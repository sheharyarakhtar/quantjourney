{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import warnings\n",
    "import pymc as pm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yf.download(['AAPL','META'], start='2023-01-01', end='2023-12-31')\n",
    "nasdaq = pd.read_html('https://en.wikipedia.org/wiki/Nasdaq-100')\n",
    "sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "nasdaq_tickers = pd.read_csv('https://www.nasdaqtrader.com/dynamic/SymDir/nasdaqtraded.txt', sep='|')\n",
    "nasdaq_tickers.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "if os.path.exists('ticker_data.csv'):\n",
    "    final_df = pd.read_csv('ticker_data.csv')\n",
    "else:\n",
    "    final_df = pd.DataFrame(columns=['Ticker', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
    "not_found = []\n",
    "\n",
    "for ticker in tqdm(sp500[0]['Symbol'].tolist()):\n",
    "    if ticker in final_df['Ticker'].tolist():\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            df = yf.download(ticker, start='2019-01-01', end='2025-03-31', multi_level_index=False)\n",
    "            df['Ticker'] = ticker\n",
    "            final_df = pd.concat([final_df, df])\n",
    "            final_df.to_csv('ticker_data.csv')\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {ticker}: {e}\")\n",
    "            not_found.append(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Define columns to exclude\n",
    "useless = ['address1',\n",
    " 'city',\n",
    " 'state',\n",
    " 'zip',\n",
    " 'country',\n",
    " 'phone',\n",
    " 'website',\n",
    " 'industry',\n",
    " 'industryKey',\n",
    " 'industryDisp',\n",
    " 'sector',\n",
    " 'sectorKey',\n",
    " 'sectorDisp',\n",
    " 'longBusinessSummary',\n",
    " 'fullTimeEmployees',\n",
    " 'companyOfficers',\n",
    " 'irWebsite',\n",
    " 'executiveTeam',\n",
    " 'maxAge',\n",
    " 'fax',\n",
    " 'displayName',\n",
    " 'marketState',\n",
    " 'longName',\n",
    " 'exchangeTimezoneName',\n",
    " 'exchangeTimezoneShortName'\n",
    " ]\n",
    "\n",
    "# Initialize empty fundamentals dataframe\n",
    "fundamentals = pd.read_csv('fundamentals.csv')\n",
    "df = pd.read_csv('ticker_data.csv')\n",
    "if fundamentals is None:\n",
    "    fundamentals = pd.DataFrame()\n",
    "    # Get fundamental data for each ticker\n",
    "    for ticker in tqdm(df['Ticker'].unique(), desc='Getting fundamental data'):\n",
    "        try:\n",
    "            tick = yf.Ticker(ticker)\n",
    "            info = tick.info\n",
    "            \n",
    "            # Get all keys except excluded ones\n",
    "            keys = [key for key in info.keys() if key not in useless]\n",
    "            values = [info[key] for key in keys]\n",
    "            \n",
    "            # Create temporary dataframe for this ticker\n",
    "            ticker_df = pd.DataFrame([values], columns=keys)\n",
    "            ticker_df['Ticker'] = ticker\n",
    "            \n",
    "            # Append to fundamentals dataframe\n",
    "            fundamentals = pd.concat([fundamentals, ticker_df], ignore_index=True)\n",
    "            fundamentals.to_csv('fundamentals.csv', index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting data for {ticker}: {e}\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('stocks.db')\n",
    "\n",
    "# Read fundamentals table into pandas DataFrame\n",
    "fundamentals = pd.read_sql_query(\"SELECT * FROM fundamentals\", conn)\n",
    "# fundamentals.to_csv('fundamentals.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(fundamentals.columns)\n",
    "\n",
    "fundamentals = pd.read_csv('fundamentals.csv')\n",
    "risks = [i for i in cols if 'Risk' in i]\n",
    "risks_df = fundamentals[['Ticker']+risks]\n",
    "\n",
    "\n",
    "\n",
    "fundamentals = fundamentals[cols]\n",
    "add = [i for i in cols if (('date' in i.lower()) or ('time' in i.lower()))] \n",
    "remove = ['isEarningsDateEstimate','firstTradeDateMilliseconds','ipoExpectedDate','nameChangeDate']\n",
    "from datetime import datetime\n",
    "# Convert timestamp columns to datetime, handling each column separately\n",
    "date_cols = [i for i in cols if ((i in add) and (i not in remove))]\n",
    "relevant_dates = fundamentals[['Ticker']+date_cols]\n",
    "for col in date_cols:\n",
    "    relevant_dates[col] = relevant_dates[col].apply(lambda x: datetime.fromtimestamp(x) if pd.notnull(x) else x)\n",
    "[cols.remove(i) for i in date_cols]\n",
    "\n",
    "\n",
    "fundamentals[cols]\n",
    "dividends = [i for i in cols if 'Dividend' in i]\n",
    "dividends_df = fundamentals[['Ticker']+dividends]\n",
    "\n",
    "[cols.remove(i) for i in dividends]\n",
    "\n",
    "L1Y = [i for i in cols if 'fifty' in i]\n",
    "L1Y_stats = fundamentals[['Ticker']+L1Y]\n",
    "[cols.remove(i) for i in L1Y]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "volume = [i for i in cols if 'volume' in i.lower()]\n",
    "volume_df = fundamentals[['Ticker']+volume]\n",
    "[cols.remove(i) for i in volume]\n",
    "\n",
    "short = [i for i in cols if 'short' in i.lower()]\n",
    "short_df = fundamentals[['Ticker']+short]\n",
    "[cols.remove(i) for i in cols]\n",
    "cols\n",
    "\n",
    "fund_df = fundamentals[cols]\n",
    "fund_df\n",
    "# [cols.remove(i) for i in fund]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ticker_df = pd.read_csv('ticker_data.csv')\n",
    "tickers = ['GIS','APA']\n",
    "%matplotlib inline\n",
    "\n",
    "for i, ticker in enumerate(tickers, 1):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    temp = ticker_df[ticker_df['Ticker'] == ticker]\n",
    "    temp = temp.set_index('Date')\n",
    "    plt.plot(temp.index, temp['Open'], label=f'{ticker} Opening Price')\n",
    "    plt.title(f'{ticker} Stock Price Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.xticks(rotation=90)  # Rotate x-axis labels 90 degrees\n",
    "    plt.xticks(plt.xticks()[0][::20])  # Show every 20th tick mark\n",
    "    plt.legend()  # Add legend since we set a label\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get GIS data and calculate required parameters\n",
    "stock = 'AAPL'\n",
    "r = 0.05  # Risk-free rate (assumed 5%)\n",
    "T = 1/12  # Time horizon (2 months)\n",
    "days = 30\n",
    "n_simulations = 1000\n",
    "\n",
    "gis_data = ticker_df[ticker_df['Ticker'] == stock].copy()\n",
    "gis_data['Returns'] = np.log(gis_data['Close']/gis_data['Close'].shift(1))\n",
    "\n",
    "# # Calculate parameters\n",
    "S0 = gis_data['Close'].iloc[-1]  \n",
    "sigma = np.std(gis_data['Returns'].dropna()) * np.sqrt(252)  # Annual volatility (252 trading days per year)\n",
    "\n",
    "\n",
    "# # Generate time points for prediction (daily for 2 months)\n",
    "t = np.linspace(0, T, days)\n",
    "\n",
    "# # Black-Scholes predicted price paths\n",
    "Z = np.random.standard_normal((n_simulations, days))\n",
    "# # Fix broadcasting issue by transposing t array\n",
    "S = S0 * np.exp((r - 0.5 * sigma**2) * t[None, :] + sigma * np.sqrt(t)[None, :] * Z)\n",
    "\n",
    "# # Calculate confidence intervals\n",
    "lower_bound = np.percentile(S, 5, axis=0)\n",
    "upper_bound = np.percentile(S, 95, axis=0)\n",
    "mean_path = np.mean(S, axis=0)\n",
    "\n",
    "# # Plot results\n",
    "plt.figure(figsize = (10,5))\n",
    "gis_data_LD = gis_data.iloc[-days:]\n",
    "gis_data_LD\n",
    "plt.plot(pd.to_datetime(gis_data_LD['Date']), gis_data_LD['Close'], label='Historical Data')\n",
    "future_dates = pd.date_range(start=pd.to_datetime(gis_data_LD.iloc[-1]['Date']), periods=days+1, freq='D')[1:]\n",
    "plt.plot(future_dates, mean_path, label='Mean Prediction')\n",
    "\n",
    "plt.fill_between(future_dates, lower_bound, upper_bound, color='gray', alpha=0.2, label=f'90% Confidence Interval')\n",
    "\n",
    "plt.title(f'{stock} Stock Price Prediction using Black-Scholes Model')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show();\n",
    "\n",
    "print(f\"Current {stock} Price: ${S0:.2f}\")\n",
    "print(f\"Predicted Price Range after 2 months:\")\n",
    "print(f\"Lower bound (5th percentile): ${lower_bound[-1]:.2f}\")\n",
    "print(f\"Mean prediction: ${mean_path[-1]:.2f}\")\n",
    "print(f\"Upper bound (95th percentile): ${upper_bound[-1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ticker_data.csv')\n",
    "df = df[df['Ticker'] == 'AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def generate_ar_process(coeffs, warmup=50, steps=150, intercept=10, noise=0.5, random=False, num_coeff = 0):\n",
    "    \"\"\"\n",
    "    Generate an autoregressive process based on provided coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "    - coeffs: List of AR coefficients\n",
    "    - warmup: Number of warmup steps to discard\n",
    "    - steps: Number of steps to keep\n",
    "    - intercept: Baseline value\n",
    "    - noise: Standard deviation of random noise\n",
    "    - random: If True, generates random coefficients between -0.5 and 0.5\n",
    "    \n",
    "    Returns:\n",
    "    - ar_data: Generated time series data\n",
    "    \"\"\"\n",
    "    if random:\n",
    "        coeffs = np.random.uniform(-0.99, 0.99, num_coeff)\n",
    "        print(f\"Generated random coefficients: {coeffs}\")\n",
    "    \n",
    "    # Number of lags is determined by the length of coeffs\n",
    "    n_lags = len(coeffs)\n",
    "    \n",
    "    # Initialize array for the entire process\n",
    "    draws = np.zeros(warmup + steps)\n",
    "    \n",
    "    # Initialize first n_lags values to the intercept\n",
    "    draws[:n_lags] = intercept\n",
    "    \n",
    "    # Generate the AR process: each value depends on previous values\n",
    "    for step in range(n_lags, warmup + steps):\n",
    "        # Start with intercept\n",
    "        value = intercept\n",
    "        \n",
    "        # Add contribution from each lag\n",
    "        for i, coef in enumerate(coeffs):\n",
    "            value += coef * draws[step - i - 1]\n",
    "        \n",
    "        # Add random noise\n",
    "        value += np.random.normal(0, noise)\n",
    "        \n",
    "        draws[step] = value\n",
    "        \n",
    "    # Return only the non-warmup portion\n",
    "    return draws[warmup:]\n",
    "\n",
    "\n",
    "def add_seasonality(data, seasonality, amplitude, noise = 1):\n",
    "    y_t = amplitude * np.sin(2 * np.pi * np.arange(len(data)) / seasonality) + np.random.normal(0, noise, len(data))\n",
    "    return y_t + data\n",
    "\n",
    "\n",
    "\n",
    "warmup = 100\n",
    "steps = 300\n",
    "intercept = 10\n",
    "noise = 5\n",
    "coeffs = [-0.9, -0.7] \n",
    "num_coeff = len(coeffs)\n",
    "random = False\n",
    "\n",
    "# Generate the AR process\n",
    "ar1_data = generate_ar_process(coeffs, warmup, steps, intercept, noise, random=random, num_coeff = num_coeff)\n",
    "\n",
    "\n",
    "slope_intercept = 2\n",
    "slope_intercept_noise = 0\n",
    "slope_coeff = 1\n",
    "slope_coeff_noise = 0.54\n",
    "\n",
    "def add_slope(data, intercept, intercept_noise, slope, slope_noise = 1, noise = 1):\n",
    "    for i in range(len(data)):\n",
    "        slope = np.random.normal(slope, slope_noise)\n",
    "        intercept = np.random.normal(intercept, intercept_noise)\n",
    "        if i == 0:\n",
    "            data[i] = data[i] + intercept + np.random.normal(0, noise)\n",
    "        else:\n",
    "            data[i] = data[i] + slope*i + np.random.normal(0, noise)\n",
    "    return data \n",
    "    # y_t = intercept + np.arange(len(data)) * slope + np.random.normal(0, intercept_noise, len(data))\n",
    "    # return y_t + ar1_data\n",
    "\n",
    "ar1_data = add_slope(ar1_data, intercept = slope_intercept, intercept_noise = slope_intercept_noise, slope = slope_coeff, slope_noise = slope_coeff_noise, noise = 0)\n",
    "\n",
    "\n",
    "seasonality = 12\n",
    "seasonality_amplitude = 120\n",
    "seasonality_amplitude_noise = 0\n",
    "seasonality_noise = 35\n",
    "\n",
    "def add_seasonality(data, seasonality, amplitude, amplitude_noise = 1, noise = 1):\n",
    "    for i in range(len(data)):\n",
    "        amplitude = np.random.normal(amplitude, amplitude_noise)\n",
    "        data[i] = data[i] + amplitude*np.sin(2*np.pi*i/seasonality) + np.random.normal(0, noise)\n",
    "    return data\n",
    "\n",
    "ar1_data = add_seasonality(ar1_data, seasonality = seasonality, amplitude = seasonality_amplitude, noise = seasonality_noise)\n",
    "\n",
    "# end_date = pd.to_datetime('2025-03-31')\n",
    "# start_date = end_date - pd.Timedelta(days=steps)\n",
    "\n",
    "# ar1_data = yf.download('AAPL', start=start_date, end=end_date, multi_level_index=False)['Close'].values\n",
    "# Create a figure to visualize the simulated time series\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "ax.set_title(f\"META Timeseries\", fontsize=15)\n",
    "ax.plot(ar1_data)  # Plot the simulated data\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx = 1\n",
    "priors = {\n",
    "    \"coefs\": {\"mu\": np.zeros(arx + 1), \"sigma\": np.ones(arx + 1) * 10, \"size\": arx + 1},\n",
    "    \"sigma\": 8,\n",
    "    \"init\": {\"mu\": 9, \"sigma\": 0.1, \"size\": 1},\n",
    "    \"alpha\": {\"mu\": -0.3, \"sigma\": 0.1},\n",
    "    \"beta\": {\"mu\": 0.3, \"sigma\": 0.2},\n",
    "    \"seasonality\": {\"mu\": 0, \"sigma\": 0.1}\n",
    "}\n",
    "add_trend = True\n",
    "add_seasonality = True\n",
    "\n",
    "with pm.Model() as AR:\n",
    "    pass\n",
    "t_data = list(range(len(ar1_data)))\n",
    "AR.add_coord(\"obs_id\", t_data)\n",
    "\n",
    "with AR:\n",
    "    t = pm.Data(\"t\", t_data, dims=\"obs_id\")\n",
    "    y = pm.Data(\"y\", ar1_data, dims=\"obs_id\")\n",
    "    coefs = pm.Normal(\"coefs\", priors[\"coefs\"][\"mu\"], priors[\"coefs\"][\"sigma\"])\n",
    "    sigma = pm.HalfNormal(\"sigma\", priors[\"sigma\"])\n",
    "    init = pm.Normal.dist(\n",
    "        priors[\"init\"][\"mu\"], priors[\"init\"][\"sigma\"], size=priors[\"init\"][\"size\"]\n",
    "    )\n",
    "    ar1 = pm.AR(\n",
    "        \"ar\",\n",
    "        coefs,                                   # The coefficients (intercept and AR coefficient)\n",
    "        sigma=sigma,                             # The noise level\n",
    "        init_dist=init,                          # Distribution for initial values\n",
    "        constant=True,                           # Include a constant term (intercept)\n",
    "        steps=t.shape[0] - (priors[\"coefs\"][\"size\"] - 1),  # Number of time steps to model\n",
    "        dims=\"obs_id\",                           # Dimension name for the time series\n",
    "    )\n",
    "    alpha = pm.Normal(\"alpha\", priors[\"alpha\"][\"mu\"], priors[\"alpha\"][\"sigma\"])\n",
    "    if add_trend:\n",
    "        beta = pm.Normal(\"beta\", priors[\"beta\"][\"mu\"], priors[\"beta\"][\"sigma\"])\n",
    "        trend = pm.Deterministic(\"trend\", alpha + beta * t, dims=\"obs_id\")\n",
    "        mu = ar1 + trend\n",
    "        if add_seasonality:\n",
    "            seasonality = pm.Normal(\"seasonality\", priors[\"seasonality\"][\"mu\"], priors[\"seasonality\"][\"sigma\"])\n",
    "            mu = mu + seasonality\n",
    "        outcome = pm.Normal(\"likelihood\", mu=mu, sigma=sigma, observed=y, dims=\"obs_id\")\n",
    "    else:\n",
    "        outcome = pm.Normal(\"likelihood\", mu=ar1, sigma=sigma, observed=y, dims=\"obs_id\")\n",
    "    idata_ar = pm.sample_prior_predictive()\n",
    "    idata_ar.extend(pm.sample(500, random_seed=100, target_accept=0.95))\n",
    "    idata_ar.extend(pm.sample_posterior_predictive(idata_ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "\n",
    "def create_figure_layout(arx):\n",
    "    \"\"\"\n",
    "    Create a figure layout for AR model visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arx : int\n",
    "        The AR order\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        The figure object\n",
    "    axes : list\n",
    "        List of axes for parameter plots\n",
    "    ax_fit : matplotlib.axes.Axes\n",
    "        Axis for model fit plot\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Set up the figure for posterior plots\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "    # Calculate how many rows we need for parameters (max 3 per row)\n",
    "    param_rows = (arx + 2 + 2) // 3  # +2 for intercept and sigma, then ceiling division\n",
    "    # Create a GridSpec layout with param_rows + 1 rows (extra row for model fit)\n",
    "    gs = fig.add_gridspec(param_rows + 1, 3, height_ratios=[1] * param_rows + [1.5])\n",
    "\n",
    "    # Create axes: one for each coefficient plus sigma\n",
    "    axes = []\n",
    "    for i in range(arx + 2):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        axes.append(fig.add_subplot(gs[row, col]))  # Parameters in rows of 3\n",
    "\n",
    "    # Add the model fit plot in the last row, spanning all columns\n",
    "    ax_fit = fig.add_subplot(gs[param_rows, :])  # bottom row spanning all columns for model fit\n",
    "    \n",
    "    return fig, axes, ax_fit\n",
    "\n",
    "def plot_ar_model_results(idata_ar, t_data, ar1_data, priors, add_trend_coeffs = False, arx=5, view_prev = None):\n",
    "    \"\"\"\n",
    "    Plot the posterior distributions and model fit for an AR model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    idata_ar : InferenceData\n",
    "        The inference data containing posterior samples and posterior predictive samples\n",
    "    t_data : list or array\n",
    "        Time points for the observed data\n",
    "    ar1_data : list or array\n",
    "        The observed time series data\n",
    "    priors : dict\n",
    "        Dictionary containing prior information\n",
    "    arx : int, optional\n",
    "        The AR order (default=5)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        The figure object with all plots\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    if add_trend_coeffs:\n",
    "        arx += 2\n",
    "    fig, axes, ax_fit = create_figure_layout(arx)\n",
    "    plot_posterior_parameters(idata_ar, axes, arx, priors, add_trend_coeffs)\n",
    "    posterior_pred = plot_model_fit(idata_ar, t_data, ar1_data, ax_fit, view_prev = view_prev)\n",
    "    plt.tight_layout()\n",
    "    plt.show() \n",
    "    return fig, posterior_pred\n",
    "def plot_posterior_parameters(idata_ar, axes, arx, priors, add_trend_coeffs = False):\n",
    "    \"\"\"\n",
    "    Plot the posterior distributions of model parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    idata_ar : InferenceData\n",
    "        The inference data containing posterior samples\n",
    "    axes : list\n",
    "        List of matplotlib axes to plot on\n",
    "    arx : int\n",
    "        The AR order\n",
    "    priors : dict\n",
    "        Dictionary containing prior information\n",
    "    \"\"\"\n",
    "    az.plot_posterior(idata_ar, var_names=[\"coefs\"], coords={\"coefs_dim_0\": 0}, ax=axes[0])\n",
    "    axes[0].set_title(\"Intercept\")\n",
    "    for i in range(1, arx-1):\n",
    "        if i <= priors[\"coefs\"][\"size\"] - 1:  # Check if coefficient exists in the model\n",
    "            az.plot_posterior(idata_ar, var_names=[\"coefs\"], coords={\"coefs_dim_0\": i}, ax=axes[i])\n",
    "            axes[i].set_title(f\"AR({i}) Coefficient\")\n",
    "    # Plot trend coefficients (alpha and beta)\n",
    "    if add_trend_coeffs:\n",
    "        az.plot_posterior(idata_ar, var_names=[\"alpha\"], ax=axes[arx-1])\n",
    "        axes[arx-1].set_title(\"Alpha (Trend Intercept)\")\n",
    "        az.plot_posterior(idata_ar, var_names=[\"beta\"], ax=axes[arx])\n",
    "        axes[arx].set_title(\"Beta (Trend Slope)\")\n",
    "\n",
    "    az.plot_posterior(idata_ar, var_names=[\"sigma\"], ax=axes[arx+1])\n",
    "    axes[arx+1].set_title(\"Sigma\")\n",
    "    # Plot sigma\n",
    "\n",
    "def plot_model_fit(idata_ar, t_data, ar1_data, ax, view_prev = None):\n",
    "    \"\"\"\n",
    "    Plot the model fit against the data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    idata_ar : InferenceData\n",
    "        The inference data containing posterior predictive samples\n",
    "    t_data : list or array\n",
    "        Time points for the observed data\n",
    "    ar1_data : list or array\n",
    "        The observed time series data\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axis to plot on\n",
    "    \"\"\"\n",
    "    if not view_prev:\n",
    "        view_prev = len(t_data)\n",
    "    ax.plot(t_data[-view_prev:], ar1_data[-view_prev:], 'o', color='black', alpha=0.6, label='Observed data')\n",
    "    posterior_pred = idata_ar.posterior_predictive[\"likelihood\"].values\n",
    "    n_samples = 100\n",
    "    sample_idx = np.random.choice(posterior_pred.shape[0] * posterior_pred.shape[1], n_samples, replace=False)\n",
    "    for idx in sample_idx:\n",
    "        chain_idx, draw_idx = idx // posterior_pred.shape[1], idx % posterior_pred.shape[1]\n",
    "        ax.plot(t_data[-view_prev:], posterior_pred[chain_idx, draw_idx][-view_prev:], color='blue', alpha=0.1)\n",
    "\n",
    "    # # Plot the mean of the posterior predictive\n",
    "    posterior_pred_mean = posterior_pred.mean(axis=(0, 1))\n",
    "    ax.plot(t_data[-view_prev:], posterior_pred_mean[-view_prev:], color='red', linewidth=2, label='Posterior mean')\n",
    "\n",
    "    ax.set_title('Model Fit')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "    return posterior_pred\n",
    "\n",
    "fig, posterior_pred = plot_ar_model_results(idata_ar, t_data, ar1_data, priors, arx=1, add_trend_coeffs = True, view_prev = 150);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata_ar, var_names=[\"coefs\"], combined=True, compact=True)\n",
    "az.plot_trace(idata_ar, var_names=[\"sigma\"], kind=\"rank_bars\", combined=True, compact=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for prediction\n",
    "def setup_future_coordinates(model, data_length, prediction_length):\n",
    "    \"\"\"Set up coordinates for future predictions.\"\"\"\n",
    "    model.add_coords({\"obs_id_fut_1\": range(data_length - 1, prediction_length, 1)})\n",
    "    model.add_coords({\"obs_id_fut\": range(data_length, prediction_length, 1)})\n",
    "    return model\n",
    "\n",
    "def plot_observed_data(ax, t_data, ar1_data, view_prev):\n",
    "    \"\"\"Plot the observed data points.\"\"\"\n",
    "    ax.plot(t_data[-view_prev:], ar1_data[-view_prev:], 'o', color='black', alpha=0.6, label='Observed data')\n",
    "\n",
    "def plot_posterior_samples(ax, t_data, posterior_pred, view_prev, n_samples=100):\n",
    "    \"\"\"Plot samples from the posterior distribution.\"\"\"\n",
    "    sample_idx = np.random.choice(posterior_pred.shape[0] * posterior_pred.shape[1], n_samples, replace=False)\n",
    "    for idx in sample_idx:\n",
    "        chain_idx, draw_idx = idx // posterior_pred.shape[1], idx % posterior_pred.shape[1]\n",
    "        ax.plot(t_data[-view_prev:], posterior_pred[chain_idx, draw_idx][-view_prev:], color='blue', alpha=0.05)\n",
    "\n",
    "def plot_posterior_mean(ax, t_data, posterior_pred, view_prev):\n",
    "    \"\"\"Plot the mean of the posterior predictive distribution.\"\"\"\n",
    "    posterior_pred_mean = posterior_pred.mean(axis=(0, 1))\n",
    "    ax.plot(t_data[-view_prev:], posterior_pred_mean[-view_prev:], color='red', linewidth=2, label='Posterior mean')\n",
    "\n",
    "def plot_future_samples(ax, future_predictions, t_future, n_samples=100):\n",
    "    \"\"\"Plot samples of future predictions.\"\"\"\n",
    "    sample_idx = np.random.choice(future_predictions.shape[0] * future_predictions.shape[1], n_samples, replace=False)\n",
    "    future_pred_mean = future_predictions.mean(axis=(0, 1))\n",
    "    \n",
    "    # Calculate max distance for normalization\n",
    "    all_distances = []\n",
    "    for idx in sample_idx:\n",
    "        chain_idx, draw_idx = idx // future_predictions.shape[1], idx % future_predictions.shape[1]\n",
    "        sample = future_predictions[chain_idx, draw_idx]\n",
    "        distance = np.mean(np.abs(sample - future_pred_mean))\n",
    "        all_distances.append(distance)\n",
    "    \n",
    "    max_distance = max(all_distances) if all_distances else 1\n",
    "    \n",
    "    for idx in sample_idx:\n",
    "        chain_idx, draw_idx = idx // future_predictions.shape[1], idx % future_predictions.shape[1]\n",
    "        sample = future_predictions[chain_idx, draw_idx]\n",
    "        \n",
    "        # Calculate distance from mean (normalized)\n",
    "        distance = np.mean(np.abs(sample - future_pred_mean))\n",
    "        norm_distance = distance / max_distance if max_distance > 0 else 0\n",
    "        \n",
    "        # Create a gradient from yellowish (closest) to reddish (middle) to blueish (furthest)\n",
    "        if norm_distance < 0.5:\n",
    "            # Yellow to red gradient (0 to 0.5)\n",
    "            ratio = norm_distance * 2  # Scale to 0-1 range\n",
    "            color = (1, 1 - ratio, 0)  # Yellow (1,1,0) to Red (1,0,0)\n",
    "        else:\n",
    "            # Red to blue gradient (0.5 to 1)\n",
    "            ratio = (norm_distance - 0.5) * 2  # Scale to 0-1 range\n",
    "            color = (1 - ratio, 0, ratio)  # Red (1,0,0) to Blue (0,0,1)\n",
    "        \n",
    "        ax.plot(t_future, sample, color=color, alpha=0.05)\n",
    "\n",
    "def plot_future_mean(ax, future_predictions, t_future):\n",
    "    \"\"\"Plot the mean of future predictions.\"\"\"\n",
    "    future_pred_mean = future_predictions.mean(axis=(0, 1))\n",
    "    ax.plot(t_future, future_pred_mean, color='green', linewidth=2, label='Future predictions (mean)', marker='o')\n",
    "\n",
    "def add_prediction_marker(ax, data_length, data_min):\n",
    "    \"\"\"Add a vertical line marking the start of predictions.\"\"\"\n",
    "    ax.axvline(x=data_length-1, color='gray', linestyle='--', alpha=0.7)\n",
    "    # ax.text(data_length, data_min, 'Prediction start', rotation=90, va='bottom', size = 10)\n",
    "\n",
    "def setup_plot_labels(ax, arx):\n",
    "    \"\"\"Set up the plot title, labels, and legend.\"\"\"\n",
    "    # Fix: plt is a module, not an Axes object. We need to use the ax parameter instead.\n",
    "    ax.set_title(f'AR({arx}) Model Predictions')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "\n",
    "def predict_future_points(AR, ar1, ar1_data, coefs, sigma, steps, idata_ar, n=10):\n",
    "    \"\"\"\n",
    "    Predict future points in the time series using the AR model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    AR : PyMC model\n",
    "        The AR model\n",
    "    ar1 : PyMC variable\n",
    "        The AR process variable\n",
    "    ar1_data : array\n",
    "        The observed data\n",
    "    coefs : PyMC variable\n",
    "        The AR coefficients\n",
    "    sigma : PyMC variable\n",
    "        The noise standard deviation\n",
    "    steps : int\n",
    "        Number of steps in the original data\n",
    "    idata_ar : InferenceData\n",
    "        The inference data from model fitting\n",
    "    n : int, optional\n",
    "        Number of future points to predict, default is 10\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (future_predictions, t_future)\n",
    "    \"\"\"\n",
    "    prediction_length = steps + n\n",
    "    view_prev = n*3\n",
    "    n_samples = 100\n",
    "    \n",
    "    # Create a new figure for the predictions\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Try to make predictions, handling the case where variables might already exist\n",
    "    try:\n",
    "        with AR:\n",
    "            # Setup coordinates for future predictions\n",
    "            AR.add_coords({\"obs_id_fut_1\": range(ar1_data.shape[0] - 1, prediction_length, 1)})\n",
    "            AR.add_coords({\"obs_id_fut\": range(ar1_data.shape[0], prediction_length, 1)})\n",
    "            try:\n",
    "                t_fut = pm.Data(\"t_fut\", list(range(ar1_data.shape[0], prediction_length, 1)))\n",
    "            except:\n",
    "                # If t_fut already exists as a Data variable, get its current value\n",
    "                try:\n",
    "                    t_fut = AR.named_vars.get('t_fut')\n",
    "                    if t_fut is None:\n",
    "                        # If it's still None, create it without using pm.Data\n",
    "                        t_fut = list(range(ar1_data.shape[0], prediction_length, 1))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error handling t_fut: {e}\")\n",
    "                    # Fallback to direct assignment\n",
    "                    t_fut = list(range(ar1_data.shape[0], prediction_length, 1))\n",
    "            # Create the future AR model directly\n",
    "            try:\n",
    "                ar1_fut = pm.AR(\n",
    "                    \"ar1_fut\",\n",
    "                    init_dist=pm.DiracDelta.dist(ar1[..., -1]),\n",
    "                    rho=coefs,\n",
    "                    sigma=sigma,\n",
    "                    constant=True,\n",
    "                    dims=\"obs_id_fut_1\",\n",
    "                )\n",
    "            except:\n",
    "                ar1_fut = AR.named_vars.get('ar1_fut')\n",
    "           \n",
    "            if add_trend and not add_seasonality:\n",
    "                print('running this? trend')\n",
    "                trend = pm.Deterministic(\"trend_fut\", alpha + beta * t_fut, dims=\"obs_id_fut\")\n",
    "                mu = ar1_fut[1:] + trend\n",
    "                yhat_fut = pm.Normal(\"yhat_fut\", mu=mu, sigma=sigma, dims=\"obs_id_fut\")\n",
    "            if add_trend and add_seasonality:\n",
    "                trend = pm.Deterministic(\"trend_fut\", alpha + beta * t_fut, dims=\"obs_id_fut\")\n",
    "                seasonality = pm.Deterministic(\"seasonality_fut\", seasonality, dims=\"obs_id_fut\")\n",
    "                mu = ar1_fut[1:] + trend + seasonality\n",
    "                yhat_fut = pm.Normal(\"yhat_fut\", mu=mu, sigma=sigma, dims=\"obs_id_fut\")\n",
    "            else:\n",
    "                print('running this?')\n",
    "                yhat_fut = pm.Normal(\"yhat_fut\", mu=ar1_fut[1:], sigma=sigma, dims=\"obs_id_fut\")\n",
    "            # Sample from the posterior predictive distribution\n",
    "            idata_preds = pm.sample_posterior_predictive(\n",
    "                idata_ar, var_names=[\"likelihood\", \"yhat_fut\"], predictions=True, random_seed=100\n",
    "            )\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        if \" already exists\" in str(e):\n",
    "            # If variables already exist, we can try to reuse them\n",
    "            with AR:\n",
    "                # Sample from the posterior predictive distribution using existing variables\n",
    "                idata_preds = pm.sample_posterior_predictive(\n",
    "                    idata_ar, var_names=[\"likelihood\", \"yhat_fut\"], predictions=True, random_seed=100\n",
    "                )\n",
    "        else:\n",
    "            # If it's a different error, re-raise it\n",
    "            raise e\n",
    "    \n",
    "    # Extract future predictions\n",
    "    future_predictions = idata_preds.predictions[\"yhat_fut\"].values\n",
    "    t_future = np.arange(ar1_data.shape[0], prediction_length)\n",
    "    \n",
    "    return future_predictions, t_future\n",
    "\n",
    "# Predict the next 10 points in the time series\n",
    "n = 15\n",
    "future_predictions, t_future = predict_future_points(AR = AR, ar1 = ar1, ar1_data = ar1_data, coefs = coefs, sigma = sigma, steps = len(ar1_data), idata_ar = idata_ar, n=10)\n",
    "view_prev = n*3\n",
    "n_samples = 500\n",
    "\n",
    "# # Create a figure and axis for plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the data and predictions\n",
    "plot_observed_data(ax, t_data, ar1_data, view_prev)\n",
    "plot_posterior_samples(ax, t_data, posterior_pred, view_prev, n_samples)\n",
    "plot_posterior_mean(ax, t_data, posterior_pred, view_prev)\n",
    "plot_future_samples(ax, future_predictions, t_future, n_samples)\n",
    "plot_future_mean(ax, future_predictions, t_future)\n",
    "add_prediction_marker(ax, ar1_data.shape[0], min(ar1_data))\n",
    "setup_plot_labels(ax, arx)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR.named_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_datareader import data, wb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader.data as web\n",
    "symbol = 'WIKI/AAPL'  # or 'AAPL.US'\n",
    "df = web.DataReader(symbol, 'quandl', '2015-01-01', '2015-01-05')\n",
    "df.loc['2015-01-02']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yf.Ticker(\"AAPL\").cashflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import warnings\n",
    "\n",
    "def download_and_analyze_hourly_data(ticker='META', start_date='2023-04-15', end_date='2024-04-01'):\n",
    "    \"\"\"\n",
    "    Download hourly stock data and calculate high/low percentage changes from open price.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ticker : str\n",
    "        Stock ticker symbol\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Processed hourly stock data with High/Low as percentage changes from open\n",
    "    \"\"\"\n",
    "    # Download hourly data\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date, \n",
    "                            multi_level_index=False, interval='1h')\n",
    "    \n",
    "    # Extract date from datetime index\n",
    "    stock_data['Date'] = stock_data.index.date\n",
    "    \n",
    "    # Convert High/Low to percentage change from Open\n",
    "    stock_data['High'] = np.round(stock_data['High']*100/stock_data['Open'],2) - 100\n",
    "    stock_data['Low'] = np.round(stock_data['Low']*100/stock_data['Open'],2) - 100\n",
    "    \n",
    "    return stock_data\n",
    "\n",
    "def calculate_hourly_range_stats(stock_data):\n",
    "    \"\"\"\n",
    "    Calculate daily statistics of hourly price ranges.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stock_data : pandas.DataFrame\n",
    "        Processed stock data with High/Low as percentage changes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Daily statistics of hourly price ranges\n",
    "    \"\"\"\n",
    "    temp = stock_data.reset_index().drop(columns=['Datetime']).set_index('Date')[['High','Low']]\n",
    "    temp['Hourly Range'] = temp['High'] - temp['Low']\n",
    "    return temp.groupby('Date').agg({'Hourly Range':['mean','std']})\n",
    "\n",
    "def plot_hourly_range_with_price_subplots(hourly_stats, daily_data, ticker='META'):\n",
    "    \"\"\"\n",
    "    Create attractive subplots showing both the standard deviation of hourly price ranges\n",
    "    and the daily stock price.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hourly_stats : pandas.DataFrame\n",
    "        Daily statistics of hourly price ranges\n",
    "    daily_data : pandas.DataFrame\n",
    "        Daily stock price data\n",
    "    ticker : str\n",
    "        Stock ticker symbol for title\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        The created figure\n",
    "    \"\"\"\n",
    "    # Extract the standard deviation of hourly range\n",
    "    hourly_range_std = hourly_stats['Hourly Range']['std']\n",
    "    \n",
    "    # Set the style for a more modern look\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6), dpi=100, facecolor='white', \n",
    "                                   sharex=True, gridspec_kw={'height_ratios': [1, 1.5]})\n",
    "    \n",
    "    # Plot hourly range std on top subplot\n",
    "    ax1.plot(hourly_range_std.index, hourly_range_std.values, \n",
    "             color='#3366CC', linewidth=2.5, alpha=0.9)\n",
    "    \n",
    "    # Add a subtle shadow/area under the line\n",
    "    ax1.fill_between(hourly_range_std.index, hourly_range_std.values, \n",
    "                     alpha=0.2, color='#3366CC')\n",
    "    \n",
    "    # Plot daily closing price on bottom subplot\n",
    "    ax2.plot(daily_data.index, daily_data['Close'], \n",
    "             color='#FF6600', linewidth=1, alpha=0.9)\n",
    "    \n",
    "    # Add candlestick-like elements to show daily range\n",
    "    for idx, row in daily_data.iterrows():\n",
    "        ax2.vlines(idx, row['Low'], row['High'], color='#FF6600', alpha=0.5, linewidth=1.5)\n",
    "    \n",
    "    # Add a subtle shadow/area under the price line\n",
    "    ax2.fill_between(daily_data.index, daily_data['Close'], \n",
    "                     min(daily_data['Low']), alpha=0.1, color='#FF6600')\n",
    "\n",
    "    # Improve the date formatting on x-axis\n",
    "    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "    ax2.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Add title and labels with better styling\n",
    "    fig.suptitle(f'{ticker} Price and Intraday Volatility', fontsize=16, fontweight='bold', y=0.98)\n",
    "    ax1.set_title('Hourly Price Range Volatility', fontsize=12, pad=10)\n",
    "    ax2.set_title('Daily Stock Price', fontsize=12, pad=10)\n",
    "    \n",
    "    ax2.set_xlabel('Date', fontsize=12, labelpad=10)\n",
    "    ax1.set_ylabel('Std Dev of Hourly Range (%)', fontsize=12, labelpad=10, color='#3366CC')\n",
    "    ax2.set_ylabel('Stock Price ($)', fontsize=12, labelpad=10, color='#FF6600')\n",
    "    \n",
    "    # Set tick colors to match the lines\n",
    "    ax1.tick_params(axis='y', colors='#3366CC')\n",
    "    ax2.tick_params(axis='y', colors='#FF6600')\n",
    "\n",
    "    # Add grid but make it subtle\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "    # Add annotations for maximum volatility value\n",
    "    max_idx = hourly_range_std.idxmax()\n",
    "    max_val = hourly_range_std.max()\n",
    "    ax1.scatter(max_idx, max_val, color='red', s=80, zorder=5)\n",
    "    ax1.annotate(f'Max Volatility: {max_val:.2f}%', \n",
    "                 xy=(max_idx, max_val),\n",
    "                 xytext=(10, -15),\n",
    "                 textcoords='offset points',\n",
    "                 fontsize=10,\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7))\n",
    "    \n",
    "    # Find corresponding price at max volatility\n",
    "    if max_idx in daily_data.index:\n",
    "        max_price = daily_data.loc[max_idx, 'Close']\n",
    "        ax2.scatter(max_idx, max_price, color='red', s=80, zorder=5)\n",
    "        ax2.annotate(f'Price: ${max_price:.2f}', \n",
    "                    xy=(max_idx, max_price),\n",
    "                    xytext=(10, 15),\n",
    "                    textcoords='offset points',\n",
    "                    fontsize=10,\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7))\n",
    "\n",
    "    # Add a subtle border\n",
    "    for ax in [ax1, ax2]:\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor('#CCCCCC')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "def plot_smoothed_data(data, ma_windows=[3, 5], ema_spans=[5], figsize=(10, 6), remove_raw_data=False):\n",
    "    # Create a copy of the data to avoid modifying the original\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # Ensure Date is set as the index for proper time series operations\n",
    "    if 'Date' in data_copy.columns:\n",
    "        data_copy.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Ensure 'Hourly Range' is numeric\n",
    "    data_copy['Hourly Range'] = pd.to_numeric(data_copy['Hourly Range'], errors='coerce')\n",
    "    \n",
    "    timeseries = {}\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Plot raw data\n",
    "    if not remove_raw_data:\n",
    "        plt.plot(data_copy.index, data_copy['Hourly Range'], color='red', linewidth=2, label='Raw Data')\n",
    "    timeseries['RawData'] = data_copy\n",
    "    \n",
    "    # Plot moving averages\n",
    "    for window in ma_windows:\n",
    "        ma = data_copy['Hourly Range'].rolling(window=window).mean()\n",
    "        plt.plot(data_copy.index, ma, linewidth=1.5, label=f'{window}-Day MA')\n",
    "        timeseries[f'{window}-Day MA'] = ma\n",
    "    \n",
    "    # Plot exponential moving averages\n",
    "    for span in ema_spans:\n",
    "        ema = data_copy['Hourly Range'].ewm(span=span, adjust=False).mean()\n",
    "        plt.plot(data_copy.index, ema, linewidth=1.5, label=f'EMA (span={span})')\n",
    "        timeseries[f'EMA (span={span})'] = ema\n",
    "    \n",
    "    plt.title('Standard Deviation of Hourly Range by Date')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Standard Deviation of Hourly Range')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return timeseries\n",
    "\n",
    "def find_best_arima_model(time_series, p_values=range(0, 6), d_values=range(0, 2), q_values=range(0, 6)):\n",
    "    \"\"\"\n",
    "    Perform a grid search to find the best ARIMA model parameters based on RMSE.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_series : pandas.Series\n",
    "        The time series data to model\n",
    "    p_values : iterable\n",
    "        Range of p values to test (AR order)\n",
    "    d_values : iterable\n",
    "        Range of d values to test (differencing)\n",
    "    q_values : iterable\n",
    "        Range of q values to test (MA order)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    statsmodels.tsa.arima.model.ARIMAResults\n",
    "        The fitted ARIMA model\n",
    "    \"\"\"\n",
    "    # Drop NaN values for model fitting\n",
    "    time_series = time_series.dropna()\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_order = None\n",
    "    best_model = None\n",
    "    best_rmse = float('inf')\n",
    "    \n",
    "    # Suppress warnings during grid search\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Split data for training and testing (80% train, 20% test)\n",
    "    train_size = int(len(time_series) * 0.8)\n",
    "    train, test = time_series[:train_size], time_series[train_size:]\n",
    "    \n",
    "    print(f\"Grid searching for best ARIMA model parameters...\")\n",
    "    \n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                try:\n",
    "                    # Fit the model\n",
    "                    model = ARIMA(train, order=(p, d, q))\n",
    "                    model_fit = model.fit()\n",
    "                    \n",
    "                    # Make predictions\n",
    "                    predictions = model_fit.forecast(steps=len(test))\n",
    "                    \n",
    "                    # Calculate RMSE\n",
    "                    rmse = sqrt(mean_squared_error(test, predictions))\n",
    "                    \n",
    "                    # Calculate AIC (Akaike Information Criterion)\n",
    "                    aic = model_fit.aic\n",
    "                    \n",
    "                    # Check if the model produces flat predictions\n",
    "                    pred_variance = np.var(predictions)\n",
    "                    if pred_variance < 0.0001:  # Very low variance indicates flat line\n",
    "                        continue\n",
    "                    \n",
    "                    # Use AIC as primary criterion, but also consider RMSE\n",
    "                    # Lower AIC indicates better model fit with penalty for complexity\n",
    "                    # This helps avoid overfitting while ensuring realistic predictions\n",
    "                    if aic < best_score or (abs(aic - best_score) < 2 and rmse < best_rmse):\n",
    "                        best_score = aic\n",
    "                        best_rmse = rmse\n",
    "                        best_order = (p, d, q)\n",
    "                        best_model = model_fit\n",
    "                        print(f\"New best ARIMA{best_order} - AIC: {aic:.2f}, RMSE: {rmse:.4f}, Var: {pred_variance:.6f}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    \n",
    "    # Re-enable warnings\n",
    "    warnings.filterwarnings(\"default\")\n",
    "    \n",
    "    if best_order is None:\n",
    "        print(\"No suitable ARIMA model found. Using default (1,1,1).\")\n",
    "        model = ARIMA(time_series, order=(1, 1, 1))\n",
    "        best_model = model.fit()\n",
    "    else:\n",
    "        print(f\"Best ARIMA model: {best_order} with AIC: {best_score:.4f}, RMSE: {best_rmse:.4f}\")\n",
    "        # Fit the best model on the full dataset\n",
    "        model = ARIMA(time_series, order=best_order)\n",
    "        best_model = model.fit()\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def forecast_time_series(timeseries, hourly_range, forecast_days=10, show_plot=True):\n",
    "    \"\"\"\n",
    "    Forecast time series data using ARIMA models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    timeseries : dict\n",
    "        Dictionary containing time series data\n",
    "    hourly_range : pandas.DataFrame\n",
    "        DataFrame containing hourly range data\n",
    "    forecast_days : int, optional\n",
    "        Number of days to forecast ahead, by default 10\n",
    "    show_plot : bool, optional\n",
    "        Whether to show the plot, by default True\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing forecasts for each time series\n",
    "    \"\"\"\n",
    "    # Get all time series except RawData\n",
    "    ts_keys = [i for i in list(timeseries.keys()) if 'RawData' not in i]\n",
    "    print(ts_keys)\n",
    "    \n",
    "    # Calculate the date range for the forecast\n",
    "    last_date = hourly_range['Date'].max()\n",
    "    date_range = pd.date_range(start=last_date + timedelta(days=1), periods=forecast_days)\n",
    "    \n",
    "    # Create a figure for all forecasts if show_plot is True\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    forecasts = {}\n",
    "    for key in ts_keys:\n",
    "        # Get the time series data\n",
    "        ts_data = timeseries[key].dropna()  # Drop NaN values for model fitting\n",
    "        \n",
    "        # Fit ARIMA model\n",
    "        model = find_best_arima_model(ts_data)\n",
    "        \n",
    "        # Forecast next period\n",
    "        forecast = model.forecast(steps=forecast_days)\n",
    "        forecasts[key] = forecast\n",
    "        \n",
    "        if show_plot:\n",
    "            # Plot only the last 10 days of historical data plus the forecast\n",
    "            last_10_days = ts_data.iloc[-10:]\n",
    "            plt.plot(last_10_days.index, last_10_days, label=f'{key} (Historical)', alpha=0.7)\n",
    "            plt.plot(date_range, forecast, '--', label=f'{key} (Forecast)')\n",
    "        \n",
    "        print(f\"Forecast for {key}:\")\n",
    "        print(forecast)\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.title('Time Series Forecasts (1 Month Ahead)')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Hourly Range')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return forecasts, date_range\n",
    "\n",
    "def filter_timeseries_keys(timeseries):\n",
    "    \"\"\"\n",
    "    Filter time series keys to get EMA and MA related keys\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    timeseries : dict\n",
    "        Dictionary containing the time series data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (ema_keys, ma_keys) - Lists of EMA and MA related keys\n",
    "    \"\"\"\n",
    "    # Get time series keys excluding raw data\n",
    "    ts_keys = [i for i in list(timeseries.keys()) if 'RawData' not in i]\n",
    "    \n",
    "    # Filter keys to get only EMA and MA related forecasts\n",
    "    ema_keys = [key for key in ts_keys if 'EMA' in key]\n",
    "    ma_keys = [key for key in ts_keys if 'MA' in key and 'EMA' not in key]\n",
    "    \n",
    "    return ema_keys, ma_keys\n",
    "\n",
    "\n",
    "def setup_forecast_plot():\n",
    "    \"\"\"\n",
    "    Create and setup the figure and axes for forecast plots\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (fig, axes) - Figure and axes objects\n",
    "    \"\"\"\n",
    "    # Create a figure with 2 subplots (one for EMA, one for MA)\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 12), facecolor='#f9f9f9')\n",
    "    fig.subplots_adjust(hspace=0.3)\n",
    "    \n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "def get_color_palettes(ema_keys, ma_keys):\n",
    "    \"\"\"\n",
    "    Generate color palettes for EMA and MA plots\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ema_keys : list\n",
    "        List of EMA related keys\n",
    "    ma_keys : list\n",
    "        List of MA related keys\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (ema_colors, ma_colors) - Color palettes for EMA and MA plots\n",
    "    \"\"\"\n",
    "    # Define color palettes for each subplot\n",
    "    ema_colors = plt.cm.viridis(np.linspace(0, 0.8, len(ema_keys)))\n",
    "    ma_colors = plt.cm.plasma(np.linspace(0, 0.8, len(ma_keys)))\n",
    "    \n",
    "    return ema_colors, ma_colors\n",
    "\n",
    "\n",
    "def plot_forecast_series(ax, key, historical_data, forecasts, date_range, color, idx):\n",
    "    \"\"\"\n",
    "    Plot a single forecast series with historical data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes to plot on\n",
    "    key : str\n",
    "        The key for the time series\n",
    "    historical_data : pandas.Series\n",
    "        Historical data for the time series\n",
    "    forecasts : dict\n",
    "        Dictionary containing forecast data\n",
    "    date_range : list\n",
    "        List of dates for the forecast period\n",
    "    color : tuple\n",
    "        RGBA color tuple for the series\n",
    "    idx : int\n",
    "        Index of the series in the keys list\n",
    "    \"\"\"\n",
    "    # Prepare combined data for smooth transition\n",
    "    combined_dates = list(historical_data.index[-10:]) + list(date_range)\n",
    "    combined_values = list(historical_data[-10:]) + list(forecasts[key])\n",
    "    \n",
    "    # Plot historical data with better styling\n",
    "    ax.plot(historical_data.index[-10:], historical_data[-10:], \n",
    "            color=color, linewidth=2.5, label=f'{key} (Historical)', alpha=0.8)\n",
    "    \n",
    "    # Plot the combined line with subtle styling\n",
    "    ax.plot(combined_dates, combined_values, color=color, alpha=0.3, linewidth=1.5)\n",
    "    \n",
    "    # Plot forecast with dashed line and better styling\n",
    "    ax.plot(date_range, forecasts[key], '--', color=color, \n",
    "            linewidth=2.5, label=f'{key} (Forecast)')\n",
    "\n",
    "\n",
    "def add_forecast_start_marker(ax, date_range):\n",
    "    \"\"\"\n",
    "    Add a vertical line and annotation for forecast start\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes to add the marker to\n",
    "    date_range : list\n",
    "        List of dates for the forecast period\n",
    "    \"\"\"\n",
    "    # Add forecast start line\n",
    "    ax.axvline(x=date_range[0], color='#444444', linestyle='--', alpha=0.7, \n",
    "               linewidth=1.5, label='Forecast Start')\n",
    "    \n",
    "    # Add annotation for forecast start\n",
    "    ax.annotate('Forecast Begins', xy=(date_range[0], ax.get_ylim()[1]*0.95),\n",
    "                xytext=(date_range[0] - pd.Timedelta(days=1), ax.get_ylim()[1]*0.95),\n",
    "                arrowprops=dict(arrowstyle='->', color='#444444', lw=1.5),\n",
    "                fontsize=10, color='#444444')\n",
    "\n",
    "\n",
    "def add_peak_marker(ax, key, forecasts, date_range, color):\n",
    "    \"\"\"\n",
    "    Add a vertical line and annotation for the peak forecast value\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes to add the marker to\n",
    "    key : str\n",
    "        The key for the time series\n",
    "    forecasts : dict\n",
    "        Dictionary containing forecast data\n",
    "    date_range : list\n",
    "        List of dates for the forecast period\n",
    "    color : tuple\n",
    "        RGBA color tuple for the series\n",
    "    \"\"\"\n",
    "    # Find peak prediction\n",
    "    predictions = list(forecasts[key].values)\n",
    "    max_index = predictions.index(max(predictions))\n",
    "    max_date = date_range[max_index]\n",
    "    max_value = predictions[max_index]\n",
    "    \n",
    "    # Add vertical line at max prediction\n",
    "    ax.axvline(x=max_date, color=color, linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "    \n",
    "    # Add annotation for peak date\n",
    "    ax.annotate(f'{key} Peak: {max_date.strftime(\"%Y-%m-%d\")}', \n",
    "                xy=(max_date, max_value),\n",
    "                xytext=(max_date + pd.Timedelta(days=1), max_value),\n",
    "                arrowprops=dict(arrowstyle='->', color=color, lw=1.5),\n",
    "                fontsize=9, color=color, fontweight='bold',\n",
    "                horizontalalignment='left')\n",
    "\n",
    "\n",
    "def add_forecast_points(ax, key, forecasts, date_range, color, idx, first_series=False):\n",
    "    \"\"\"\n",
    "    Add markers and date labels for forecast points\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes to add the points to\n",
    "    key : str\n",
    "        The key for the time series\n",
    "    forecasts : dict\n",
    "        Dictionary containing forecast data\n",
    "    date_range : list\n",
    "        List of dates for the forecast period\n",
    "    color : tuple\n",
    "        RGBA color tuple for the series\n",
    "    idx : int\n",
    "        Index of the series in the keys list\n",
    "    first_series : bool, optional\n",
    "        Whether this is the first series (to add date labels)\n",
    "    \"\"\"\n",
    "    # Add date annotations for all forecast points\n",
    "    for i, date in enumerate(date_range):\n",
    "        value = forecasts[key].iloc[i] if hasattr(forecasts[key], 'iloc') else forecasts[key][i]\n",
    "        \n",
    "        # Add small markers at each forecast point\n",
    "        ax.plot(date, value, 'o', color=color, markersize=4)\n",
    "        \n",
    "        # Add date labels only for the first series to avoid clutter\n",
    "        if first_series:\n",
    "            ax.annotate(date.strftime(\"%m-%d\"), xy=(date, value),\n",
    "                       xytext=(0, -15), textcoords='offset points',\n",
    "                       ha='center', va='top', fontsize=8, rotation=45)\n",
    "\n",
    "\n",
    "def enhance_subplot_styling(ax, title, date_range):\n",
    "    \"\"\"\n",
    "    Enhance the styling of a subplot\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes to style\n",
    "    title : str\n",
    "        The title for the subplot\n",
    "    date_range : list\n",
    "        List of dates for the forecast period\n",
    "    \"\"\"\n",
    "    # Set title and labels\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Hourly Range', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add shaded region for forecast period\n",
    "    ax.axvspan(date_range[0], date_range[-1], alpha=0.1, color='gray', label='Forecast Period')\n",
    "    \n",
    "    # Improve legend\n",
    "    ax.legend(loc='upper left', frameon=True, framealpha=0.9, fontsize=10)\n",
    "    \n",
    "    # Add a subtle border\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "\n",
    "\n",
    "def plot_ema_ma_forecasts(timeseries, forecasts, date_range, forecast_days=10):\n",
    "    \"\"\"\n",
    "    Create subplots for EMA and MA forecasts with improved aesthetics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    timeseries : dict\n",
    "        Dictionary containing the time series data\n",
    "    forecasts : dict\n",
    "        Dictionary containing forecast data for each time series\n",
    "    date_range : list\n",
    "        List of dates for the forecast period\n",
    "    forecast_days : int, optional\n",
    "        Number of days to forecast ahead (default: 10)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        The figure object containing the plots\n",
    "    \"\"\"\n",
    "    # Filter keys and setup plot\n",
    "    ema_keys, ma_keys = filter_timeseries_keys(timeseries)\n",
    "    fig, axes = setup_forecast_plot()\n",
    "    ema_colors, ma_colors = get_color_palettes(ema_keys, ma_keys)\n",
    "    \n",
    "    # Plot EMA forecasts\n",
    "    for idx, key in enumerate(ema_keys):\n",
    "        historical_data = timeseries[key].dropna()\n",
    "        \n",
    "        # Plot the series\n",
    "        plot_forecast_series(axes[0], key, historical_data, forecasts, date_range, ema_colors[idx], idx)\n",
    "        \n",
    "        # Add forecast start marker only for the first series\n",
    "        if idx == 0:\n",
    "            add_forecast_start_marker(axes[0], date_range)\n",
    "        \n",
    "        # Add peak marker and forecast points\n",
    "        add_peak_marker(axes[0], key, forecasts, date_range, ema_colors[idx])\n",
    "        add_forecast_points(axes[0], key, forecasts, date_range, ema_colors[idx], idx, idx == 0)\n",
    "    \n",
    "    # Plot MA forecasts\n",
    "    for idx, key in enumerate(ma_keys):\n",
    "        historical_data = timeseries[key].dropna()\n",
    "        \n",
    "        # Plot the series\n",
    "        plot_forecast_series(axes[1], key, historical_data, forecasts, date_range, ma_colors[idx], idx)\n",
    "        \n",
    "        # Add forecast start marker only for the first series\n",
    "        if idx == 0:\n",
    "            add_forecast_start_marker(axes[1], date_range)\n",
    "        \n",
    "        # Add peak marker and forecast points\n",
    "        add_peak_marker(axes[1], key, forecasts, date_range, ma_colors[idx])\n",
    "        add_forecast_points(axes[1], key, forecasts, date_range, ma_colors[idx], idx, idx == 0)\n",
    "    \n",
    "    # Enhance styling for both subplots\n",
    "    enhance_subplot_styling(axes[0], 'EMA Forecasts (10 Days Ahead)', date_range)\n",
    "    enhance_subplot_styling(axes[1], 'MA Forecasts (10 Days Ahead)', date_range)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configuration dictionary for stock analysis\n",
    "# config = {\n",
    "#     'ticker': 'META',\n",
    "#     'start_date': '2025-02-01',\n",
    "#     'end_date': '2025-04-06',\n",
    "#     'filter_date': '2025-03-01',\n",
    "#     'ma_windows': [2, 5, 10],\n",
    "#     'ema_spans': [2, 5, 10],\n",
    "#     'forecast_days': 10\n",
    "# }\n",
    "\n",
    "# # Download and process data\n",
    "# meta_data = download_and_analyze_hourly_data(config['ticker'], config['start_date'], config['end_date'])\n",
    "# hourly_stats = calculate_hourly_range_stats(meta_data)\n",
    "# daily_data = yf.download(config['ticker'], start=config['start_date'], end=config['end_date'], \n",
    "#                          interval='1d', multi_level_index=False)\n",
    "\n",
    "# # Create and display the combined subplot visualization\n",
    "# fig = plot_hourly_range_with_price_subplots(hourly_stats, daily_data, config['ticker'])\n",
    "# plt.show()\n",
    "\n",
    "# # Process hourly range data\n",
    "# hourly_range = hourly_stats['Hourly Range']['std']\n",
    "# hourly_range = hourly_range.reset_index()\n",
    "# hourly_range.rename(columns={'std': 'Hourly Range'}, inplace=True)\n",
    "# hourly_range = hourly_range[hourly_range['Date'] >= pd.to_datetime(config['filter_date']).date()]\n",
    "\n",
    "# # Generate time series and forecasts\n",
    "# timeseries = plot_smoothed_data(hourly_range, ma_windows=config['ma_windows'], \n",
    "#                                ema_spans=config['ema_spans'], remove_raw_data=True)\n",
    "# forecasts, date_range = forecast_time_series(timeseries, hourly_range, forecast_days=config['forecast_days'], show_plot=False)\n",
    "fig = plot_ema_ma_forecasts(timeseries, forecasts, date_range)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = daily_data.reset_index()\n",
    "temp['Close'] = temp['Close']/temp['Open']\n",
    "# Convert Date column in hourly_range to datetime if it's an int\n",
    "hourly_range_copy = hourly_range.copy()\n",
    "if hourly_range_copy['Date'].dtype == 'int64':\n",
    "    hourly_range_copy['Date'] = pd.to_datetime(hourly_range_copy['Date'])\n",
    "# Or alternatively use pd.concat as suggested by the error\n",
    "result = pd.concat([temp, hourly_range_copy], axis=1, join='inner')\n",
    "result[['Close','Hourly Range']].corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
