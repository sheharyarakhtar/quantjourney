{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "yf.download(['AAPL','META'], start='2023-01-01', end='2023-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "nasdaq = pd.read_html('https://en.wikipedia.org/wiki/Nasdaq-100')\n",
    "sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq_tickers = pd.read_csv('https://www.nasdaqtrader.com/dynamic/SymDir/nasdaqtraded.txt', sep='|')\n",
    "nasdaq_tickers.head()\n",
    "# # Clean up the data\n",
    "# nasdaq_tickers = nasdaq_tickers[nasdaq_tickers['Test Issue'] == 'N']\n",
    "# nasdaq_tickers = nasdaq_tickers[nasdaq_tickers['ETF'] == 'N']\n",
    "# # Get just the symbols\n",
    "# symbols = nasdaq_tickers['NASDAQ Symbol'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "if os.path.exists('ticker_data.csv'):\n",
    "    final_df = pd.read_csv('ticker_data.csv')\n",
    "else:\n",
    "    final_df = pd.DataFrame(columns=['Ticker', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])\n",
    "not_found = []\n",
    "\n",
    "for ticker in tqdm(sp500[0]['Symbol'].tolist()):\n",
    "    if ticker in final_df['Ticker'].tolist():\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            df = yf.download(ticker, start='2019-01-01', end='2025-03-31', multi_level_index=False)\n",
    "            df['Ticker'] = ticker\n",
    "            final_df = pd.concat([final_df, df])\n",
    "            final_df.to_csv('ticker_data.csv')\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {ticker}: {e}\")\n",
    "            not_found.append(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Define columns to exclude\n",
    "useless = ['address1',\n",
    " 'city',\n",
    " 'state',\n",
    " 'zip',\n",
    " 'country',\n",
    " 'phone',\n",
    " 'website',\n",
    " 'industry',\n",
    " 'industryKey',\n",
    " 'industryDisp',\n",
    " 'sector',\n",
    " 'sectorKey',\n",
    " 'sectorDisp',\n",
    " 'longBusinessSummary',\n",
    " 'fullTimeEmployees',\n",
    " 'companyOfficers',\n",
    " 'irWebsite',\n",
    " 'executiveTeam',\n",
    " 'maxAge',\n",
    " 'fax',\n",
    " 'displayName',\n",
    " 'marketState',\n",
    " 'longName',\n",
    " 'exchangeTimezoneName',\n",
    " 'exchangeTimezoneShortName'\n",
    " ]\n",
    "\n",
    "# Initialize empty fundamentals dataframe\n",
    "fundamentals = pd.DataFrame()\n",
    "\n",
    "# Load ticker data\n",
    "df = pd.read_csv('ticker_data.csv')\n",
    "\n",
    "# Get fundamental data for each ticker\n",
    "for ticker in tqdm(df['Ticker'].unique(), desc='Getting fundamental data'):\n",
    "    try:\n",
    "        tick = yf.Ticker(ticker)\n",
    "        info = tick.info\n",
    "        \n",
    "        # Get all keys except excluded ones\n",
    "        keys = [key for key in info.keys() if key not in useless]\n",
    "        values = [info[key] for key in keys]\n",
    "        \n",
    "        # Create temporary dataframe for this ticker\n",
    "        ticker_df = pd.DataFrame([values], columns=keys)\n",
    "        ticker_df['Ticker'] = ticker\n",
    "        \n",
    "        # Append to fundamentals dataframe\n",
    "        fundamentals = pd.concat([fundamentals, ticker_df], ignore_index=True)\n",
    "        fundamentals.to_csv('fundamentals.csv', index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting data for {ticker}: {e}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect('stocks.db')\n",
    "\n",
    "# Read fundamentals table into pandas DataFrame\n",
    "fundamentals = pd.read_sql_query(\"SELECT * FROM fundamentals\", conn)\n",
    "fundamentals.to_csv('fundamentals.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cols = list(fundamentals.columns)\n",
    "\n",
    "fundamentals = pd.read_csv('fundamentals.csv')\n",
    "risks = [i for i in cols if 'Risk' in i]\n",
    "risks_df = fundamentals[['Ticker']+risks]\n",
    "\n",
    "\n",
    "\n",
    "fundamentals = fundamentals[cols]\n",
    "add = [i for i in cols if (('date' in i.lower()) or ('time' in i.lower()))] \n",
    "remove = ['isEarningsDateEstimate','firstTradeDateMilliseconds','ipoExpectedDate','nameChangeDate']\n",
    "from datetime import datetime\n",
    "# Convert timestamp columns to datetime, handling each column separately\n",
    "date_cols = [i for i in cols if ((i in add) and (i not in remove))]\n",
    "relevant_dates = fundamentals[['Ticker']+date_cols]\n",
    "for col in date_cols:\n",
    "    relevant_dates[col] = relevant_dates[col].apply(lambda x: datetime.fromtimestamp(x) if pd.notnull(x) else x)\n",
    "[cols.remove(i) for i in date_cols]\n",
    "\n",
    "\n",
    "fundamentals[cols]\n",
    "dividends = [i for i in cols if 'Dividend' in i]\n",
    "dividends_df = fundamentals[['Ticker']+dividends]\n",
    "\n",
    "[cols.remove(i) for i in dividends]\n",
    "\n",
    "L1Y = [i for i in cols if 'fifty' in i]\n",
    "L1Y_stats = fundamentals[['Ticker']+L1Y]\n",
    "[cols.remove(i) for i in L1Y]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "volume = [i for i in cols if 'volume' in i.lower()]\n",
    "volume_df = fundamentals[['Ticker']+volume]\n",
    "[cols.remove(i) for i in volume]\n",
    "\n",
    "short = [i for i in cols if 'short' in i.lower()]\n",
    "short_df = fundamentals[['Ticker']+short]\n",
    "[cols.remove(i) for i in cols]\n",
    "cols\n",
    "\n",
    "fund_df = fundamentals[cols]\n",
    "fund_df\n",
    "# [cols.remove(i) for i in fund]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ticker_df = pd.read_csv('ticker_data.csv')\n",
    "tickers = ['GIS','APA']\n",
    "%matplotlib inline\n",
    "\n",
    "for i, ticker in enumerate(tickers, 1):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    temp = ticker_df[ticker_df['Ticker'] == ticker]\n",
    "    temp = temp.set_index('Date')\n",
    "    plt.plot(temp.index, temp['Open'], label=f'{ticker} Opening Price')\n",
    "    plt.title(f'{ticker} Stock Price Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.xticks(rotation=90)  # Rotate x-axis labels 90 degrees\n",
    "    plt.xticks(plt.xticks()[0][::20])  # Show every 20th tick mark\n",
    "    plt.legend()  # Add legend since we set a label\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get GIS data and calculate required parameters\n",
    "stock = 'AAPL'\n",
    "r = 0.05  # Risk-free rate (assumed 5%)\n",
    "T = 1/12  # Time horizon (2 months)\n",
    "days = 30\n",
    "n_simulations = 1000\n",
    "\n",
    "gis_data = ticker_df[ticker_df['Ticker'] == stock].copy()\n",
    "gis_data['Returns'] = np.log(gis_data['Close']/gis_data['Close'].shift(1))\n",
    "\n",
    "# # Calculate parameters\n",
    "S0 = gis_data['Close'].iloc[-1]  \n",
    "sigma = np.std(gis_data['Returns'].dropna()) * np.sqrt(252)  # Annual volatility (252 trading days per year)\n",
    "\n",
    "\n",
    "# # Generate time points for prediction (daily for 2 months)\n",
    "t = np.linspace(0, T, days)\n",
    "\n",
    "# # Black-Scholes predicted price paths\n",
    "Z = np.random.standard_normal((n_simulations, days))\n",
    "# # Fix broadcasting issue by transposing t array\n",
    "S = S0 * np.exp((r - 0.5 * sigma**2) * t[None, :] + sigma * np.sqrt(t)[None, :] * Z)\n",
    "\n",
    "# # Calculate confidence intervals\n",
    "lower_bound = np.percentile(S, 5, axis=0)\n",
    "upper_bound = np.percentile(S, 95, axis=0)\n",
    "mean_path = np.mean(S, axis=0)\n",
    "\n",
    "# # Plot results\n",
    "plt.figure(figsize = (10,5))\n",
    "gis_data_LD = gis_data.iloc[-days:]\n",
    "gis_data_LD\n",
    "plt.plot(pd.to_datetime(gis_data_LD['Date']), gis_data_LD['Close'], label='Historical Data')\n",
    "future_dates = pd.date_range(start=pd.to_datetime(gis_data_LD.iloc[-1]['Date']), periods=days+1, freq='D')[1:]\n",
    "plt.plot(future_dates, mean_path, label='Mean Prediction')\n",
    "\n",
    "plt.fill_between(future_dates, lower_bound, upper_bound, color='gray', alpha=0.2, label=f'90% Confidence Interval')\n",
    "\n",
    "plt.title(f'{stock} Stock Price Prediction using Black-Scholes Model')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show();\n",
    "\n",
    "print(f\"Current {stock} Price: ${S0:.2f}\")\n",
    "print(f\"Predicted Price Range after 2 months:\")\n",
    "print(f\"Lower bound (5th percentile): ${lower_bound[-1]:.2f}\")\n",
    "print(f\"Mean prediction: ${mean_path[-1]:.2f}\")\n",
    "print(f\"Upper bound (95th percentile): ${upper_bound[-1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ticker_data.csv')\n",
    "df = df[df['Ticker'] == 'AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ar(intercept, coef1, coef2, noise=0.3, *, warmup=10, steps=200):\n",
    "    \"\"\"\n",
    "    Simulate an autoregressive (AR) time series process of order 2.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    intercept : float\n",
    "        The constant term in the AR model\n",
    "    coef1 : float\n",
    "        Coefficient for the first lag (AR(1) term)\n",
    "    coef2 : float\n",
    "        Coefficient for the second lag (AR(2) term)\n",
    "    noise : float, default=0.3\n",
    "        Standard deviation of the random noise\n",
    "    warmup : int, default=10\n",
    "        Number of initial steps to discard to let the process stabilize\n",
    "    steps : int, default=200\n",
    "        Number of time steps to simulate after warmup\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        The simulated AR process values (excluding warmup period)\n",
    "    \"\"\"\n",
    "    # Create an array to hold all values (including warmup)\n",
    "    draws = np.zeros(warmup + steps)\n",
    "    \n",
    "    # Initialize first two values to the intercept (starting point)\n",
    "    draws[:2] = intercept\n",
    "    \n",
    "    # Generate the AR process: each value depends on previous values\n",
    "    for step in range(2, warmup + steps):\n",
    "        # AR(2) formula: y_t = intercept + coef1*y_{t-1} + coef2*y_{t-2} + noise\n",
    "        draws[step] = (\n",
    "            intercept                        # Constant term\n",
    "            + coef1 * draws[step - 1]        # First lag effect\n",
    "            + coef2 * draws[step - 2]        # Second lag effect\n",
    "            + np.random.normal(0, noise)     # Random noise/shock\n",
    "        )\n",
    "    \n",
    "    # Return only the values after warmup period\n",
    "    return draws[warmup:]\n",
    "\n",
    "\n",
    "# Generate an AR(1) process with negative coefficient (-0.9)\n",
    "# This will create an oscillating pattern (values bounce up and down)\n",
    "# The intercept is 10 (baseline value), and coef2=0 means it's only AR(1), not AR(2)\n",
    "\n",
    "ar1_data = simulate_ar(10, -0.9, 0)\n",
    "\n",
    "# Create a figure to visualize the simulated time series\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "ax.set_title(\"Generated Autoregressive Timeseries\", fontsize=15)\n",
    "ax.plot(ar1_data);  # Plot the simulated data\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up a dictionary for the specification of our priors\n",
    "## We set up the dictionary to specify size of the AR coefficients in\n",
    "## case we want to vary the AR lags.\n",
    "priors = {\n",
    "    \"coefs\": {\"mu\": [10, 0.2], \"sigma\": [0.1, 0.1], \"size\": 2},\n",
    "    \"sigma\": 8,\n",
    "    \"init\": {\"mu\": 9, \"sigma\": 0.1, \"size\": 1},\n",
    "}\n",
    "\n",
    "## Initialise the model\n",
    "with pm.Model() as AR:\n",
    "    pass\n",
    "\n",
    "## Define the time interval for fitting the data\n",
    "t_data = list(range(len(ar1_data)))\n",
    "## Add the time interval as a mutable coordinate to the model to allow for future predictions\n",
    "AR.add_coord(\"obs_id\", t_data)\n",
    "\n",
    "with AR:\n",
    "    ## Data containers to enable prediction\n",
    "    t = pm.Data(\"t\", t_data, dims=\"obs_id\")\n",
    "    y = pm.Data(\"y\", ar1_data, dims=\"obs_id\")\n",
    "\n",
    "    # The first coefficient will be the constant term but we need to set priors for each coefficient in the AR process\n",
    "    coefs = pm.Normal(\"coefs\", priors[\"coefs\"][\"mu\"], priors[\"coefs\"][\"sigma\"])\n",
    "    sigma = pm.HalfNormal(\"sigma\", priors[\"sigma\"])\n",
    "    # We need one init variable for each lag, hence size is variable too\n",
    "    init = pm.Normal.dist(\n",
    "        priors[\"init\"][\"mu\"], priors[\"init\"][\"sigma\"], size=priors[\"init\"][\"size\"]\n",
    "    )\n",
    "    # Steps of the AR model minus the lags required\n",
    "    ar1 = pm.AR(\n",
    "        \"ar\",\n",
    "        coefs,\n",
    "        sigma=sigma,\n",
    "        init_dist=init,\n",
    "        constant=True,\n",
    "        steps=t.shape[0] - (priors[\"coefs\"][\"size\"] - 1),\n",
    "        dims=\"obs_id\",\n",
    "    )\n",
    "\n",
    "    # The Likelihood\n",
    "    outcome = pm.Normal(\"likelihood\", mu=ar1, sigma=sigma, observed=y, dims=\"obs_id\")\n",
    "    ## Sampling\n",
    "    idata_ar = pm.sample_prior_predictive()\n",
    "    idata_ar.extend(pm.sample(1000, random_seed=100, target_accept=0.95))\n",
    "    idata_ar.extend(pm.sample_posterior_predictive(idata_ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior distributions and model fit\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the figure for posterior plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot the posterior distributions of model parameters\n",
    "az.plot_posterior(idata_ar, var_names=[\"coefs\"], ax=axes[0])\n",
    "az.plot_posterior(idata_ar, var_names=[\"sigma\"], ax=axes[1])\n",
    "\n",
    "# Plot the model fit against the data\n",
    "ax = axes[2]\n",
    "# Plot the original data\n",
    "ax.plot(t_data, ar1_data, 'o', color='black', alpha=0.6, label='Observed data')\n",
    "\n",
    "# Plot the posterior predictive samples\n",
    "posterior_pred = idata_ar.posterior_predictive[\"likelihood\"].values\n",
    "# Take a subset of posterior samples for clarity\n",
    "n_samples = 50\n",
    "sample_idx = np.random.choice(posterior_pred.shape[0] * posterior_pred.shape[1], n_samples, replace=False)\n",
    "for idx in sample_idx:\n",
    "    chain_idx, draw_idx = idx // posterior_pred.shape[1], idx % posterior_pred.shape[1]\n",
    "    ax.plot(t_data, posterior_pred[chain_idx, draw_idx], color='blue', alpha=0.1)\n",
    "\n",
    "# Plot the mean of the posterior predictive\n",
    "posterior_pred_mean = posterior_pred.mean(axis=(0, 1))\n",
    "ax.plot(t_data, posterior_pred_mean, color='red', linewidth=2, label='Posterior mean')\n",
    "\n",
    "ax.set_title('Model Fit')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Value')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot trace plots to check convergence\n",
    "az.plot_trace(idata_ar, var_names=[\"coefs\", \"sigma\"])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data[meta_data.High == meta_data.High.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def download_and_analyze_hourly_data(ticker='META', start_date='2023-04-15', end_date='2024-04-01'):\n",
    "    \"\"\"\n",
    "    Download hourly stock data and calculate high/low percentage changes from open price.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ticker : str\n",
    "        Stock ticker symbol\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Processed hourly stock data with High/Low as percentage changes from open\n",
    "    \"\"\"\n",
    "    # Download hourly data\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date, \n",
    "                            multi_level_index=False, interval='1h')\n",
    "    \n",
    "    # Extract date from datetime index\n",
    "    stock_data['Date'] = stock_data.index.date\n",
    "    \n",
    "    # Convert High/Low to percentage change from Open\n",
    "    stock_data['High'] = np.round(stock_data['High']*100/stock_data['Open'],2) - 100\n",
    "    stock_data['Low'] = np.round(stock_data['Low']*100/stock_data['Open'],2) - 100\n",
    "    \n",
    "    return stock_data\n",
    "\n",
    "def calculate_hourly_range_stats(stock_data):\n",
    "    \"\"\"\n",
    "    Calculate daily statistics of hourly price ranges.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    stock_data : pandas.DataFrame\n",
    "        Processed stock data with High/Low as percentage changes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Daily statistics of hourly price ranges\n",
    "    \"\"\"\n",
    "    temp = stock_data.reset_index().drop(columns=['Datetime']).set_index('Date')[['High','Low']]\n",
    "    temp['Hourly Range'] = temp['High'] - temp['Low']\n",
    "    return temp.groupby('Date').agg({'Hourly Range':['mean','std']})\n",
    "\n",
    "def plot_hourly_range_with_price_subplots(hourly_stats, daily_data, ticker='META'):\n",
    "    \"\"\"\n",
    "    Create attractive subplots showing both the standard deviation of hourly price ranges\n",
    "    and the daily stock price.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hourly_stats : pandas.DataFrame\n",
    "        Daily statistics of hourly price ranges\n",
    "    daily_data : pandas.DataFrame\n",
    "        Daily stock price data\n",
    "    ticker : str\n",
    "        Stock ticker symbol for title\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    matplotlib.figure.Figure\n",
    "        The created figure\n",
    "    \"\"\"\n",
    "    # Extract the standard deviation of hourly range\n",
    "    hourly_range_std = hourly_stats['Hourly Range']['std']\n",
    "    \n",
    "    # Set the style for a more modern look\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6), dpi=100, facecolor='white', \n",
    "                                   sharex=True, gridspec_kw={'height_ratios': [1, 1.5]})\n",
    "    \n",
    "    # Plot hourly range std on top subplot\n",
    "    ax1.plot(hourly_range_std.index, hourly_range_std.values, \n",
    "             color='#3366CC', linewidth=2.5, alpha=0.9)\n",
    "    \n",
    "    # Add a subtle shadow/area under the line\n",
    "    ax1.fill_between(hourly_range_std.index, hourly_range_std.values, \n",
    "                     alpha=0.2, color='#3366CC')\n",
    "    \n",
    "    # Plot daily closing price on bottom subplot\n",
    "    ax2.plot(daily_data.index, daily_data['Close'], \n",
    "             color='#FF6600', linewidth=1, alpha=0.9)\n",
    "    \n",
    "    # Add candlestick-like elements to show daily range\n",
    "    for idx, row in daily_data.iterrows():\n",
    "        ax2.vlines(idx, row['Low'], row['High'], color='#FF6600', alpha=0.5, linewidth=1.5)\n",
    "    \n",
    "    # Add a subtle shadow/area under the price line\n",
    "    ax2.fill_between(daily_data.index, daily_data['Close'], \n",
    "                     min(daily_data['Low']), alpha=0.1, color='#FF6600')\n",
    "\n",
    "    # Improve the date formatting on x-axis\n",
    "    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "    ax2.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Add title and labels with better styling\n",
    "    fig.suptitle(f'{ticker} Price and Intraday Volatility', fontsize=16, fontweight='bold', y=0.98)\n",
    "    ax1.set_title('Hourly Price Range Volatility', fontsize=12, pad=10)\n",
    "    ax2.set_title('Daily Stock Price', fontsize=12, pad=10)\n",
    "    \n",
    "    ax2.set_xlabel('Date', fontsize=12, labelpad=10)\n",
    "    ax1.set_ylabel('Std Dev of Hourly Range (%)', fontsize=12, labelpad=10, color='#3366CC')\n",
    "    ax2.set_ylabel('Stock Price ($)', fontsize=12, labelpad=10, color='#FF6600')\n",
    "    \n",
    "    # Set tick colors to match the lines\n",
    "    ax1.tick_params(axis='y', colors='#3366CC')\n",
    "    ax2.tick_params(axis='y', colors='#FF6600')\n",
    "\n",
    "    # Add grid but make it subtle\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "    # Add annotations for maximum volatility value\n",
    "    max_idx = hourly_range_std.idxmax()\n",
    "    max_val = hourly_range_std.max()\n",
    "    ax1.scatter(max_idx, max_val, color='red', s=80, zorder=5)\n",
    "    ax1.annotate(f'Max Volatility: {max_val:.2f}%', \n",
    "                 xy=(max_idx, max_val),\n",
    "                 xytext=(10, -15),\n",
    "                 textcoords='offset points',\n",
    "                 fontsize=10,\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7))\n",
    "    \n",
    "    # Find corresponding price at max volatility\n",
    "    if max_idx in daily_data.index:\n",
    "        max_price = daily_data.loc[max_idx, 'Close']\n",
    "        ax2.scatter(max_idx, max_price, color='red', s=80, zorder=5)\n",
    "        ax2.annotate(f'Price: ${max_price:.2f}', \n",
    "                    xy=(max_idx, max_price),\n",
    "                    xytext=(10, 15),\n",
    "                    textcoords='offset points',\n",
    "                    fontsize=10,\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7))\n",
    "\n",
    "    # Add a subtle border\n",
    "    for ax in [ax1, ax2]:\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor('#CCCCCC')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set date range\n",
    "    start_date = '2025-02-01'\n",
    "    end_date = '2025-04-05'\n",
    "    ticker = 'META'\n",
    "    \n",
    "    # Download and process hourly data\n",
    "    meta_data = download_and_analyze_hourly_data(ticker, start_date, end_date)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    hourly_stats = calculate_hourly_range_stats(meta_data)\n",
    "    \n",
    "    # Download daily data\n",
    "    daily_data = yf.download(ticker, start=start_date, end=end_date, interval='1d', multi_level_index=False)\n",
    "    \n",
    "    # Create and display the combined subplot visualization\n",
    "    fig = plot_hourly_range_with_price_subplots(hourly_stats, daily_data, ticker)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_smoothed_data(data, ma_windows=[3, 5], ema_spans=[5], figsize=(10, 6), remove_raw_data=False):\n",
    "    # Create a copy of the data to avoid modifying the original\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # Ensure Date is set as the index for proper time series operations\n",
    "    if 'Date' in data_copy.columns:\n",
    "        data_copy.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Ensure 'Hourly Range' is numeric\n",
    "    data_copy['Hourly Range'] = pd.to_numeric(data_copy['Hourly Range'], errors='coerce')\n",
    "    \n",
    "    timeseries = {}\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Plot raw data\n",
    "    if not remove_raw_data:\n",
    "        plt.plot(data_copy.index, data_copy['Hourly Range'], color='red', linewidth=2, label='Raw Data')\n",
    "    timeseries['RawData'] = data_copy\n",
    "    \n",
    "    # Plot moving averages\n",
    "    for window in ma_windows:\n",
    "        ma = data_copy['Hourly Range'].rolling(window=window).mean()\n",
    "        plt.plot(data_copy.index, ma, linewidth=1.5, label=f'{window}-Day MA')\n",
    "        timeseries[f'{window}-Day MA'] = ma\n",
    "    \n",
    "    # Plot exponential moving averages\n",
    "    for span in ema_spans:\n",
    "        ema = data_copy['Hourly Range'].ewm(span=span, adjust=False).mean()\n",
    "        plt.plot(data_copy.index, ema, linewidth=1.5, label=f'EMA (span={span})')\n",
    "        timeseries[f'EMA (span={span})'] = ema\n",
    "    \n",
    "    plt.title('Standard Deviation of Hourly Range by Date')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Standard Deviation of Hourly Range')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return timeseries\n",
    "\n",
    "# Example usage with different parameters\n",
    "\n",
    "hourly_range = hourly_stats['Hourly Range']['std']\n",
    "hourly_range = hourly_range.reset_index()\n",
    "hourly_range.rename(columns={'std': 'Hourly Range'}, inplace=True)\n",
    "\n",
    "hourly_range = hourly_range[hourly_range['Date'] >= pd.to_datetime('2025-03-01').date()]\n",
    "timeseries = plot_smoothed_data(hourly_range, ma_windows=[2,5,10], ema_spans=[2,5,10], remove_raw_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import warnings\n",
    "\n",
    "def find_best_arima_model(time_series, p_values=range(0, 6), d_values=range(0, 2), q_values=range(0, 6)):\n",
    "    \"\"\"\n",
    "    Perform a grid search to find the best ARIMA model parameters based on RMSE.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_series : pandas.Series\n",
    "        The time series data to model\n",
    "    p_values : iterable\n",
    "        Range of p values to test (AR order)\n",
    "    d_values : iterable\n",
    "        Range of d values to test (differencing)\n",
    "    q_values : iterable\n",
    "        Range of q values to test (MA order)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    statsmodels.tsa.arima.model.ARIMAResults\n",
    "        The fitted ARIMA model\n",
    "    \"\"\"\n",
    "    # Drop NaN values for model fitting\n",
    "    time_series = time_series.dropna()\n",
    "    \n",
    "    best_score = float('inf')\n",
    "    best_order = None\n",
    "    best_model = None\n",
    "    best_rmse = float('inf')\n",
    "    \n",
    "    # Suppress warnings during grid search\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Split data for training and testing (80% train, 20% test)\n",
    "    train_size = int(len(time_series) * 0.8)\n",
    "    train, test = time_series[:train_size], time_series[train_size:]\n",
    "    \n",
    "    print(f\"Grid searching for best ARIMA model parameters...\")\n",
    "    \n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                try:\n",
    "                    # Fit the model\n",
    "                    model = ARIMA(train, order=(p, d, q))\n",
    "                    model_fit = model.fit()\n",
    "                    \n",
    "                    # Make predictions\n",
    "                    predictions = model_fit.forecast(steps=len(test))\n",
    "                    \n",
    "                    # Calculate RMSE\n",
    "                    rmse = sqrt(mean_squared_error(test, predictions))\n",
    "                    \n",
    "                    # Calculate AIC (Akaike Information Criterion)\n",
    "                    aic = model_fit.aic\n",
    "                    \n",
    "                    # Check if the model produces flat predictions\n",
    "                    pred_variance = np.var(predictions)\n",
    "                    if pred_variance < 0.0001:  # Very low variance indicates flat line\n",
    "                        continue\n",
    "                    \n",
    "                    # Use AIC as primary criterion, but also consider RMSE\n",
    "                    # Lower AIC indicates better model fit with penalty for complexity\n",
    "                    # This helps avoid overfitting while ensuring realistic predictions\n",
    "                    if aic < best_score or (abs(aic - best_score) < 2 and rmse < best_rmse):\n",
    "                        best_score = aic\n",
    "                        best_rmse = rmse\n",
    "                        best_order = (p, d, q)\n",
    "                        best_model = model_fit\n",
    "                        print(f\"New best ARIMA{best_order} - AIC: {aic:.2f}, RMSE: {rmse:.4f}, Var: {pred_variance:.6f}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    \n",
    "    # Re-enable warnings\n",
    "    warnings.filterwarnings(\"default\")\n",
    "    \n",
    "    if best_order is None:\n",
    "        print(\"No suitable ARIMA model found. Using default (1,1,1).\")\n",
    "        model = ARIMA(time_series, order=(1, 1, 1))\n",
    "        best_model = model.fit()\n",
    "    else:\n",
    "        print(f\"Best ARIMA model: {best_order} with AIC: {best_score:.4f}, RMSE: {best_rmse:.4f}\")\n",
    "        # Fit the best model on the full dataset\n",
    "        model = ARIMA(time_series, order=best_order)\n",
    "        best_model = model.fit()\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def forecast_time_series(timeseries, hourly_range, forecast_days=10, show_plot=True):\n",
    "    \"\"\"\n",
    "    Forecast time series data using ARIMA models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    timeseries : dict\n",
    "        Dictionary containing time series data\n",
    "    hourly_range : pandas.DataFrame\n",
    "        DataFrame containing hourly range data\n",
    "    forecast_days : int, optional\n",
    "        Number of days to forecast ahead, by default 10\n",
    "    show_plot : bool, optional\n",
    "        Whether to show the plot, by default True\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing forecasts for each time series\n",
    "    \"\"\"\n",
    "    # Get all time series except RawData\n",
    "    ts_keys = [i for i in list(timeseries.keys()) if 'RawData' not in i]\n",
    "    print(ts_keys)\n",
    "    \n",
    "    # Calculate the date range for the forecast\n",
    "    last_date = hourly_range['Date'].max()\n",
    "    date_range = pd.date_range(start=last_date + timedelta(days=1), periods=forecast_days)\n",
    "    \n",
    "    # Create a figure for all forecasts if show_plot is True\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    forecasts = {}\n",
    "    for key in ts_keys:\n",
    "        # Get the time series data\n",
    "        ts_data = timeseries[key].dropna()  # Drop NaN values for model fitting\n",
    "        \n",
    "        # Fit ARIMA model\n",
    "        model = find_best_arima_model(ts_data)\n",
    "        \n",
    "        # Forecast next period\n",
    "        forecast = model.forecast(steps=forecast_days)\n",
    "        forecasts[key] = forecast\n",
    "        \n",
    "        if show_plot:\n",
    "            # Plot only the last 10 days of historical data plus the forecast\n",
    "            last_10_days = ts_data.iloc[-10:]\n",
    "            plt.plot(last_10_days.index, last_10_days, label=f'{key} (Historical)', alpha=0.7)\n",
    "            plt.plot(date_range, forecast, '--', label=f'{key} (Forecast)')\n",
    "        \n",
    "        print(f\"Forecast for {key}:\")\n",
    "        print(forecast)\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.title('Time Series Forecasts (1 Month Ahead)')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Hourly Range')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return forecasts, date_range\n",
    "forecasts, date_range = forecast_time_series(timeseries, hourly_range, forecast_days=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for EMA and MA forecasts with improved aesthetics\n",
    "ts_keys = [i for i in list(timeseries.keys()) if 'RawData' not in i]\n",
    "\n",
    "# Filter keys to get only EMA and MA related forecasts\n",
    "ema_keys = [key for key in ts_keys if 'EMA' in key]\n",
    "ma_keys = [key for key in ts_keys if 'MA' in key and 'EMA' not in key]\n",
    "\n",
    "# Create a figure with 2 subplots (one for EMA, one for MA)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 12), facecolor='#f9f9f9')\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "\n",
    "# Define color palettes for each subplot\n",
    "ema_colors = plt.cm.viridis(np.linspace(0, 0.8, len(ema_keys)))\n",
    "ma_colors = plt.cm.plasma(np.linspace(0, 0.8, len(ma_keys)))\n",
    "\n",
    "# Plot EMA forecasts on the first subplot\n",
    "for idx, key in enumerate(ema_keys):\n",
    "    ax = axes[0]\n",
    "    \n",
    "    historical_data = timeseries[key].dropna()\n",
    "    combined_dates = list(historical_data.index[-10:]) + list(date_range)\n",
    "    combined_values = list(historical_data[-10:]) + list(forecasts[key])\n",
    "    \n",
    "    # Plot historical data with better styling\n",
    "    ax.plot(historical_data.index[-10:], historical_data[-10:], \n",
    "            color=ema_colors[idx], linewidth=2.5, label=f'{key} (Historical)', alpha=0.8)\n",
    "    \n",
    "    # Plot the combined line with subtle styling\n",
    "    ax.plot(combined_dates, combined_values, color=ema_colors[idx], alpha=0.3, linewidth=1.5)\n",
    "    \n",
    "    # Plot forecast with dashed line and better styling\n",
    "    ax.plot(date_range, forecasts[key], '--', color=ema_colors[idx], \n",
    "            linewidth=2.5, label=f'{key} (Forecast)')\n",
    "    \n",
    "    # Add forecast start line (only for the first key to avoid duplication)\n",
    "    if idx == 0:\n",
    "        ax.axvline(x=date_range[0], color='#444444', linestyle='--', alpha=0.7, \n",
    "                   linewidth=1.5, label='Forecast Start')\n",
    "        \n",
    "        # Add annotation for forecast start\n",
    "        ax.annotate('Forecast Begins', xy=(date_range[0], ax.get_ylim()[1]*0.95),\n",
    "                    xytext=(date_range[0] - pd.Timedelta(days=1), ax.get_ylim()[1]*0.95),\n",
    "                    arrowprops=dict(arrowstyle='->', color='#444444', lw=1.5),\n",
    "                    fontsize=10, color='#444444')\n",
    "    \n",
    "    # Add vertical line at max prediction with annotation\n",
    "    predictions = list(forecasts[key].values)\n",
    "    max_index = predictions.index(max(predictions))\n",
    "    max_date = date_range[max_index]\n",
    "    max_value = predictions[max_index]\n",
    "    \n",
    "    ax.axvline(x=max_date, color=ema_colors[idx], linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "    \n",
    "    # Add annotation for peak date\n",
    "    ax.annotate(f'{key} Peak: {max_date.strftime(\"%Y-%m-%d\")}', \n",
    "                xy=(max_date, max_value),\n",
    "                xytext=(max_date + pd.Timedelta(days=1), max_value + idx*0.01),\n",
    "                arrowprops=dict(arrowstyle='->', color=ema_colors[idx], lw=1.5),\n",
    "                fontsize=9, color=ema_colors[idx], fontweight='bold',\n",
    "                horizontalalignment='left')\n",
    "    \n",
    "    # Add date annotations for all forecast points\n",
    "    for i, date in enumerate(date_range):\n",
    "        value = forecasts[key].iloc[i] if hasattr(forecasts[key], 'iloc') else forecasts[key][i]\n",
    "        # Add small markers at each forecast point\n",
    "        ax.plot(date, value, 'o', color=ema_colors[idx], markersize=4)\n",
    "        \n",
    "        # Add date labels only for the first series to avoid clutter\n",
    "        if idx == 0:\n",
    "            ax.annotate(date.strftime(\"%m-%d\"), xy=(date, value),\n",
    "                       xytext=(0, -15), textcoords='offset points',\n",
    "                       ha='center', va='top', fontsize=8, rotation=45)\n",
    "\n",
    "# Plot MA forecasts on the second subplot\n",
    "for idx, key in enumerate(ma_keys):\n",
    "    ax = axes[1]\n",
    "    \n",
    "    historical_data = timeseries[key].dropna()\n",
    "    combined_dates = list(historical_data.index[-10:]) + list(date_range)\n",
    "    combined_values = list(historical_data[-10:]) + list(forecasts[key])\n",
    "    \n",
    "    # Plot historical data with better styling\n",
    "    ax.plot(historical_data.index[-10:], historical_data[-10:], \n",
    "            color=ma_colors[idx], linewidth=2.5, label=f'{key} (Historical)', alpha=0.8)\n",
    "    \n",
    "    # Plot the combined line with subtle styling\n",
    "    ax.plot(combined_dates, combined_values, color=ma_colors[idx], alpha=0.3, linewidth=1.5)\n",
    "    \n",
    "    # Plot forecast with dashed line and better styling\n",
    "    ax.plot(date_range, forecasts[key], '--', color=ma_colors[idx], \n",
    "            linewidth=2.5, label=f'{key} (Forecast)')\n",
    "    \n",
    "    # Add forecast start line (only for the first key to avoid duplication)\n",
    "    if idx == 0:\n",
    "        ax.axvline(x=date_range[0], color='#444444', linestyle='--', alpha=0.7, \n",
    "                   linewidth=1.5, label='Forecast Start')\n",
    "        \n",
    "        # Add annotation for forecast start\n",
    "        ax.annotate('Forecast Begins', xy=(date_range[0], ax.get_ylim()[1]*0.95),\n",
    "                    xytext=(date_range[0] - pd.Timedelta(days=1), ax.get_ylim()[1]*0.95),\n",
    "                    arrowprops=dict(arrowstyle='->', color='#444444', lw=1.5),\n",
    "                    fontsize=10, color='#444444')\n",
    "    \n",
    "    # Add vertical line at max prediction with annotation\n",
    "    predictions = list(forecasts[key].values)\n",
    "    max_index = predictions.index(max(predictions))\n",
    "    max_date = date_range[max_index]\n",
    "    max_value = predictions[max_index]\n",
    "    \n",
    "    ax.axvline(x=max_date, color=ma_colors[idx], linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "    \n",
    "    # Add annotation for peak date\n",
    "    ax.annotate(f'{key} Peak: {max_date.strftime(\"%Y-%m-%d\")}', \n",
    "                xy=(max_date, max_value),\n",
    "                xytext=(max_date + pd.Timedelta(days=1), max_value + idx*0.01),\n",
    "                arrowprops=dict(arrowstyle='->', color=ma_colors[idx], lw=1.5),\n",
    "                fontsize=9, color=ma_colors[idx], fontweight='bold',\n",
    "                horizontalalignment='left')\n",
    "    \n",
    "    # Add date annotations for all forecast points\n",
    "    for i, date in enumerate(date_range):\n",
    "        value = forecasts[key].iloc[i] if hasattr(forecasts[key], 'iloc') else forecasts[key][i]\n",
    "        # Add small markers at each forecast point\n",
    "        ax.plot(date, value, 'o', color=ma_colors[idx], markersize=4)\n",
    "        \n",
    "        # Add date labels only for the first series to avoid clutter\n",
    "        if idx == 0:\n",
    "            ax.annotate(date.strftime(\"%m-%d\"), xy=(date, value),\n",
    "                       xytext=(0, -15), textcoords='offset points',\n",
    "                       ha='center', va='top', fontsize=8, rotation=45)\n",
    "\n",
    "# Enhance plot styling for both subplots\n",
    "for i, title in enumerate(['EMA Forecasts (10 Days Ahead)', 'MA Forecasts (10 Days Ahead)']):\n",
    "    ax = axes[i]\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Hourly Range', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add shaded region for forecast period\n",
    "    ax.axvspan(date_range[0], date_range[-1], alpha=0.1, color='gray', label='Forecast Period')\n",
    "    \n",
    "    # Improve legend\n",
    "    ax.legend(loc='upper left', frameon=True, framealpha=0.9, fontsize=10)\n",
    "    \n",
    "    # Add a subtle border\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,5]\n",
    "# Find the index of the maximum value in list a\n",
    "max_index = a.index(max(a))\n",
    "print(f\"The index of the maximum value in a is: {max_index}\")\n",
    "print(f\"The maximum value is: {a[max_index]}\")\n",
    "\n",
    "\n",
    "predictions = list(forecasts[key].values[1:])\n",
    "max_index = predictions.index(max(predictions))\n",
    "forecasts[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8')  # Updated to use the correct style name\n",
    "# Alternative options: 'ggplot', 'fivethirtyeight', or 'seaborn-v0_8-darkgrid'\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def plot_straddle_payoff():\n",
    "    # Parameters\n",
    "    strike_price = 100\n",
    "    premium_call = 5\n",
    "    premium_put = 5\n",
    "    total_premium = premium_call + premium_put\n",
    "    \n",
    "    # Generate price range\n",
    "    stock_prices = np.linspace(70, 130, 1000)\n",
    "    \n",
    "    # Calculate payoffs\n",
    "    call_payoff = np.maximum(stock_prices - strike_price, 0) - premium_call\n",
    "    put_payoff = np.maximum(strike_price - stock_prices, 0) - premium_put\n",
    "    total_payoff = call_payoff + put_payoff\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(stock_prices, call_payoff, '--', label='Call Option', alpha=0.7)\n",
    "    plt.plot(stock_prices, put_payoff, '--', label='Put Option', alpha=0.7)\n",
    "    plt.plot(stock_prices, total_payoff, 'b-', label='Straddle Payoff', linewidth=2)\n",
    "    \n",
    "    # Add horizontal line at y=0\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=strike_price, color='k', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title('Straddle Strategy Payoff Profile', fontsize=14, pad=15)\n",
    "    plt.xlabel('Stock Price at Expiration', fontsize=12)\n",
    "    plt.ylabel('Profit/Loss', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add break-even points annotation\n",
    "    break_even_lower = strike_price - total_premium\n",
    "    break_even_upper = strike_price + total_premium\n",
    "    plt.plot([break_even_lower, break_even_upper], [0, 0], 'ro')\n",
    "    plt.annotate(f'Break-even: {break_even_lower:.1f}', \n",
    "                xy=(break_even_lower, 0), xytext=(break_even_lower-5, -10),\n",
    "                arrowprops=dict(facecolor='red', shrink=0.05))\n",
    "    plt.annotate(f'Break-even: {break_even_upper:.1f}', \n",
    "                xy=(break_even_upper, 0), xytext=(break_even_upper+5, -10),\n",
    "                arrowprops=dict(facecolor='red', shrink=0.05))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()  # Explicitly show the plot\n",
    "\n",
    "def plot_iv_impact():\n",
    "    # Parameters\n",
    "    strike_price = 100\n",
    "    time_to_expiry = 1.0  # 1 year\n",
    "    risk_free_rate = 0.05\n",
    "    spot_price = 100\n",
    "    \n",
    "    def black_scholes_call(S, K, T, r, sigma):\n",
    "        d1 = (np.log(S/K) + (r + sigma**2/2)*T) / (sigma*np.sqrt(T))\n",
    "        d2 = d1 - sigma*np.sqrt(T)\n",
    "        return S*norm.cdf(d1) - K*np.exp(-r*T)*norm.cdf(d2)\n",
    "    \n",
    "    # Generate IV range\n",
    "    iv_range = np.linspace(0.1, 1.0, 100)\n",
    "    option_prices = [black_scholes_call(spot_price, strike_price, time_to_expiry, \n",
    "                                      risk_free_rate, iv) for iv in iv_range]\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(iv_range * 100, option_prices, 'b-', linewidth=2)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title('Impact of Implied Volatility on Option Price', fontsize=14, pad=15)\n",
    "    plt.xlabel('Implied Volatility (%)', fontsize=12)\n",
    "    plt.ylabel('Option Price', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotation for key points\n",
    "    mid_point = len(iv_range)//2\n",
    "    plt.annotate('Higher IV = Higher Premium',\n",
    "                xy=(iv_range[mid_point]*100, option_prices[mid_point]),\n",
    "                xytext=(iv_range[mid_point]*100+10, option_prices[mid_point]+5),\n",
    "                arrowprops=dict(facecolor='blue', shrink=0.05))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()  # Explicitly show the plot\n",
    "\n",
    "# Call the functions to generate plots\n",
    "plot_straddle_payoff()\n",
    "plot_iv_impact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
