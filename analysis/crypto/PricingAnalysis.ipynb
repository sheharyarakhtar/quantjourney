{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De-BTCing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('history.csv')\n",
    "df = df[['Coin','dateTime','open','close']]\n",
    "final_df = pd.DataFrame()\n",
    "for coin in tqdm(df['Coin'].unique(), desc=\"Processing coins\"):\n",
    "    temp_df = df[df.Coin == coin]\n",
    "    temp_df['open_pct'] = np.round(temp_df['open'].pct_change()*100,2)\n",
    "    temp_df['close_pct'] = np.round(temp_df['close'].pct_change()*100,2)\n",
    "    # Apply moving average smoothing to percentage changes\n",
    "    # Default window size of 7 days for weekly smoothing\n",
    "    window_size = 7\n",
    "    \n",
    "    # Create smoothed versions of the percentage changes\n",
    "    temp_df['open_pct_smooth'] = temp_df['open_pct'].rolling(window=window_size, min_periods=1).mean()\n",
    "    temp_df['close_pct_smooth'] = temp_df['close_pct'].rolling(window=window_size, min_periods=1).mean()\n",
    "    \n",
    "    # Fill any NaN values that might be created at the beginning of the series\n",
    "    temp_df['open_pct_smooth'] = temp_df['open_pct_smooth'].fillna(temp_df['open_pct'])\n",
    "    temp_df['close_pct_smooth'] = temp_df['close_pct_smooth'].fillna(temp_df['close_pct'])\n",
    "    final_df = pd.concat([final_df, temp_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Correlation Coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 60\n",
    "\n",
    "final_df['dateTime'] = pd.to_datetime(final_df['dateTime'], format='%Y-%m-%d %H:%M:%S')\n",
    "max_date = final_df.dateTime.max()\n",
    "from datetime import timedelta\n",
    "min_date = max_date - timedelta(days=n_days)\n",
    "independent = ['BTCUSDT','ETHUSDT','BNBUSDT','XRPUSDT']\n",
    "correlation_df = final_df[['Coin','dateTime', 'close_pct']].dropna()\n",
    "correlation_df = correlation_df[correlation_df.dateTime > min_date].pivot_table(index = 'dateTime', columns = ['Coin'], values = 'close_pct').corr()[independent]\n",
    "correlation_df = correlation_df.sort_values('BTCUSDT',ascending = False).dropna()\n",
    "# Remove coins with very low correlation (less than 0.1 magnitude) with all independent coins\n",
    "correlation_threshold = 0.8\n",
    "\n",
    "\n",
    "correlation_df = correlation_df[(correlation_df.abs() >= correlation_threshold).any(axis=1)]\n",
    "correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_prepare_data(analysis_df):\n",
    "    \"\"\"Clean data and prepare for analysis\"\"\"\n",
    "    cleaned_df = analysis_df.dropna()\n",
    "    return cleaned_df.copy()\n",
    "\n",
    "def extract_btc_data(analysis_df):\n",
    "    \"\"\"Extract and prepare BTC data\"\"\"\n",
    "    btc = analysis_df[analysis_df.Coin == 'BTCUSDT']\n",
    "    btc.drop(columns=['Coin', 'open', 'close'], inplace=True)\n",
    "    return btc\n",
    "\n",
    "def extract_nonbtc_data(analysis_df):\n",
    "    \"\"\"Extract non-BTC data\"\"\"\n",
    "    nonbtc = analysis_df[~(analysis_df.Coin == 'BTCUSDT')].drop(columns=['open', 'close'])\n",
    "    return nonbtc\n",
    "\n",
    "def merge_btc_nonbtc_data(nonbtc, btc):\n",
    "    \"\"\"Merge BTC and non-BTC datasets\"\"\"\n",
    "    return nonbtc.merge(btc, on='dateTime', suffixes=['', '_btc'])\n",
    "\n",
    "def calculate_debtcfied_movements(joined_df):\n",
    "    \"\"\"Calculate BTC-independent price movements\"\"\"\n",
    "    joined_df['de_btcfied_open'] = joined_df['open_pct'] - joined_df['open_pct_btc']\n",
    "    joined_df['de_btcfied_close'] = joined_df['close_pct'] - joined_df['close_pct_btc']\n",
    "    return joined_df\n",
    "\n",
    "\n",
    "def plot_debtcfied_comparison(joined_df, selected_coins, n_days):\n",
    "    \"\"\"Plot BTC-independent price movement comparison\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    \n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot each coin\n",
    "    for coin in selected_coins:\n",
    "        plotdata = joined_df[joined_df.Coin == coin].tail(n_days)\n",
    "        \n",
    "        # Convert dateTime to datetime if needed\n",
    "        if not pd.api.types.is_datetime64_any_dtype(plotdata['dateTime']):\n",
    "            plotdata['dateTime'] = pd.to_datetime(plotdata['dateTime'])\n",
    "        \n",
    "        # Plot with label for legend\n",
    "        ax.plot(plotdata['dateTime'], plotdata['de_btcfied_close'], linewidth=2, label=coin)\n",
    "    \n",
    "    # Format the plot\n",
    "    ax.set_title(f'BTC-Independent Price Movement Comparison', fontsize=14)\n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('De-BTCfied Close (%)', fontsize=12)\n",
    "    \n",
    "    # Format x-axis dates\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add legend and grid\n",
    "    ax.legend(title='Coins', loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Main execution\n",
    "n_days = 60\n",
    "\n",
    "# Process data step by step\n",
    "analysis = clean_and_prepare_data(final_df)\n",
    "btc = extract_btc_data(analysis)\n",
    "nonbtc = extract_nonbtc_data(analysis)\n",
    "joined = merge_btc_nonbtc_data(nonbtc, btc)\n",
    "joined = calculate_debtcfied_movements(joined)\n",
    "joined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_coins_by_performance_and_data(joined_df, n_days, min_date, min_datapoints=10):\n",
    "    \"\"\"Filter coins based on performance and minimum data requirements\"\"\"\n",
    "    \n",
    "    # Only keep coins that meet both criteria\n",
    "    filtered_coins = [coin for coin in joined_df.Coin.unique() \n",
    "                     if len(joined_df[joined_df.Coin == coin]) > min_datapoints \n",
    "                     and len(joined_df[(joined_df.Coin == coin) & (joined_df.dateTime >= min_date)]) > min_datapoints]\n",
    "\n",
    "    stats = []\n",
    "    for coin in filtered_coins:\n",
    "        coin_data = joined_df[joined_df.Coin == coin].tail(n_days)\n",
    "        coin_stats = {\n",
    "            'coin': coin,\n",
    "            'coin_mean': coin_data['de_btcfied_close'].mean(),\n",
    "            'coin_sd': coin_data['de_btcfied_close'].std(),\n",
    "            'coin_n': len(coin_data),\n",
    "            'min_date': coin_data.dateTime.min(),\n",
    "            'max_date': coin_data.dateTime.max()\n",
    "        }   \n",
    "        stats.append(coin_stats)\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "def conduct_hypothesis_test(stats_df, threshold=0, sided=1, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Conduct hypothesis test for each coin's mean change vs threshold\n",
    "    \n",
    "    Parameters:\n",
    "    - stats_df: DataFrame with coin statistics (coin, coin_mean, coin_sd, coin_n)\n",
    "    - threshold: The threshold value to test against (default 0)\n",
    "    - sided: 1 for one-sided test (mean > threshold), 2 for two-sided test\n",
    "    - alpha: Significance level (default 0.05)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with test results for each coin\n",
    "    \"\"\"\n",
    "    from scipy import stats as scipy_stats\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for _, row in stats_df.iterrows():\n",
    "        coin = row['coin']\n",
    "        mean = row['coin_mean']\n",
    "        sd = row['coin_sd']\n",
    "        n = row['coin_n']\n",
    "        \n",
    "        # Calculate t-statistic\n",
    "        t_stat = (mean - threshold) / (sd / np.sqrt(n))\n",
    "        \n",
    "        # Calculate p-value based on test type\n",
    "        if sided == 1:\n",
    "            # One-sided test: H0: mean <= threshold, H1: mean > threshold\n",
    "            p_value = 1 - scipy_stats.t.cdf(t_stat, df=n-1)\n",
    "            test_type = \"one-sided (mean > threshold)\"\n",
    "        else:\n",
    "            # Two-sided test: H0: mean = threshold, H1: mean != threshold\n",
    "            p_value = 2 * (1 - scipy_stats.t.cdf(abs(t_stat), df=n-1))\n",
    "            test_type = \"two-sided\"\n",
    "        \n",
    "        # Determine if result is significant\n",
    "        significant = p_value < alpha\n",
    "        \n",
    "        result = {\n",
    "            'coin': coin,\n",
    "            'mean': mean,\n",
    "            'threshold': threshold,\n",
    "            'n': n,\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': significant,\n",
    "            'test_type': test_type,\n",
    "            'alpha': alpha\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nHypothesis Test Results ({test_type}):\")\n",
    "    print(f\"Threshold: {threshold}\")\n",
    "    print(f\"Significance level: {alpha}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for _, row in results_df[results_df.significant == True].iterrows():\n",
    "        significance = \"***\" if row['significant'] else \"\"\n",
    "        print(f\"{row['coin']:10} | Mean: {row['mean']:7.3f} | t-stat: {row['t_statistic']:7.3f} | p-value: {row['p_value']:7.4f} {significance}\")\n",
    "    \n",
    "    significant_coins = results_df[results_df['significant']]['coin'].tolist()\n",
    "    if significant_coins:\n",
    "        print(f\"\\nCoins with significant evidence (p < {alpha}): {', '.join(significant_coins)}\")\n",
    "    else:\n",
    "        print(f\"\\nNo coins show significant evidence at α = {alpha}\")\n",
    "    \n",
    "    return results_df\n",
    "joined['dateTime'] = pd.to_datetime(joined['dateTime'], format='%Y-%m-%d %H:%M:%S')\n",
    "max_date = joined.dateTime.max()\n",
    "from datetime import timedelta\n",
    "min_date = max_date - timedelta(days=n_days)\n",
    "\n",
    "# Filter coins and create plot\n",
    "stats = filter_coins_by_performance_and_data(joined, n_days, min_date = min_date, min_datapoints=0.40*n_days)\n",
    "statistics = conduct_hypothesis_test(stats, threshold=0, sided=1, alpha = 0.15)\n",
    "statistics.head()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_coins = statistics[statistics.p_value < 0.11]['coin'].values\n",
    "\n",
    "# First plot: close_pct for selected coins\n",
    "plt.figure(figsize=(12, 6))\n",
    "for coin in list(selected_coins) + ['BTCUSDT']:\n",
    "    coin_data = final_df[final_df.Coin == coin].tail(n_days)\n",
    "    if coin == 'BTCUSDT':\n",
    "        plt.plot(coin_data['dateTime'], coin_data['close_pct'], label=f'{coin}', alpha=1, linewidth=3)\n",
    "    else:\n",
    "        plt.plot(coin_data['dateTime'], coin_data['close_pct'], label=f'{coin}', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Close Percentage (%)', fontsize=12)\n",
    "plt.title('Close Percentage for Selected Coins', fontsize=14)\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Second plot: de_btcfied_close for selected coins\n",
    "plt.figure(figsize=(12, 6))\n",
    "for coin in selected_coins:\n",
    "    coin_data = joined[joined.Coin == coin].tail(n_days)\n",
    "    plt.plot(coin_data['dateTime'], coin_data['de_btcfied_close'], label=f'{coin}', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('De-BTCfied Close (%)', fontsize=12)\n",
    "plt.title('De-BTCfied Close for Selected Coins', fontsize=14)\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the data table\n",
    "joined[joined.Coin.isin(selected_coins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure for the bell curves (normal distribution plots)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# For each selected coin, plot its distribution\n",
    "for coin in selected_coins:\n",
    "    # Get the data for this coin\n",
    "    coin_data = joined[joined.Coin == coin]['de_btcfied_close'].dropna().tail(n_days)\n",
    "    plt.hist(coin_data, bins=8, alpha=0.5, label=coin, density=True)\n",
    "\n",
    "# Improve formatting\n",
    "plt.title('Distribution of BTC-Independent Price Movements', fontsize=14)\n",
    "plt.xlabel('De-BTCfied Close (%)', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(title='Coins', loc='best')\n",
    "\n",
    "# Add grid and tight layout\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_1samp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# independent = ['BTCUSDT','ETHUSDT','BNBUSDT']\n",
    "final_df['dateTime'] = pd.to_datetime(joined['dateTime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Step 1: Prepare the data for regression analysis\n",
    "final_df.dropna(inplace = True)\n",
    "# Get BTC and ETH data as market factors\n",
    "independent_variables = pd.DataFrame()\n",
    "for coin_symbol in independent:\n",
    "    temp_independent = final_df[final_df.Coin == coin_symbol][['dateTime', 'close_pct']].rename(columns={'close_pct': coin_symbol})\n",
    "    if independent_variables.empty:\n",
    "        independent_variables = temp_independent\n",
    "    else:\n",
    "        independent_variables = pd.merge(independent_variables, temp_independent, on='dateTime', how='inner')\n",
    "\n",
    "def filter_coins_by_data(joined_df, n_days, min_date, min_datapoints=10):\n",
    "    \"\"\"Filter coins based on performance and minimum data requirements\"\"\"\n",
    "    \n",
    "    # Only keep coins that meet both criteria\n",
    "    filtered_coins = [coin for coin in joined_df.Coin.unique() \n",
    "                     if len(joined_df[joined_df.Coin == coin]) > min_datapoints \n",
    "                     and len(joined_df[(joined_df.Coin == coin) & (joined_df.dateTime >= min_date)]) > min_datapoints]\n",
    "\n",
    "    return filtered_coins\n",
    "\n",
    "\n",
    "max_date = joined.dateTime.max()\n",
    "from datetime import timedelta\n",
    "min_date = max_date - timedelta(days=n_days)\n",
    "filtered_coins =  filter_coins_by_data(final_df[~final_df.Coin.isin(independent)], n_days = 60, min_date = min_date, min_datapoints=10)\n",
    "\n",
    "\n",
    "# Get target coin data\n",
    "residuals_df = pd.DataFrame()\n",
    "results = pd.DataFrame()\n",
    "for coin in tqdm(filtered_coins):\n",
    "    target_coin_data = final_df[final_df.Coin == coin][['dateTime', 'close_pct']].rename(columns={'close_pct': coin})\n",
    "    # Merge target coin with market factors\n",
    "    regression_data = pd.merge(target_coin_data, independent_variables, on='dateTime', how='inner')\n",
    "\n",
    "    # Step 2: Choose your target coin\n",
    "    y = regression_data[coin]\n",
    "\n",
    "    # # # Step 3: Choose market factors (BTC, ETH, etc.)\n",
    "    X = regression_data[independent]\n",
    "    # X = sm.add_constant(X)  # Adds intercept\n",
    "\n",
    "    # # # Step 4: Fit linear model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # # # Step 5: Get residuals (these are your de-BTCified returns)\n",
    "    residuals = model.resid\n",
    "    t_stat, p_value = ttest_1samp(residuals, 0, alternative='greater')\n",
    "\n",
    "    # Update residuals dataframe\n",
    "    temp_residuals = pd.DataFrame({\n",
    "        'dateTime': regression_data['dateTime'],\n",
    "        'coin': coin,\n",
    "        'residual': residuals\n",
    "    })\n",
    "    \n",
    "    if residuals_df.empty:\n",
    "        residuals_df = temp_residuals\n",
    "    else:\n",
    "        residuals_df = pd.concat([residuals_df, temp_residuals], ignore_index=True)\n",
    "    \n",
    "    # Update results dataframe\n",
    "    temp_results = pd.DataFrame({\n",
    "        'coin': [coin],\n",
    "        'r_squared': [model.rsquared],\n",
    "        'adj_r_squared': [model.rsquared_adj],\n",
    "        'f_statistic': [model.fvalue],\n",
    "        'f_pvalue': [model.f_pvalue],\n",
    "        'aic': [model.aic],\n",
    "        'bic': [model.bic],\n",
    "        'residual_mean': [residuals.mean()],\n",
    "        'residual_std': [residuals.std()],\n",
    "        't_statistic': [t_stat],\n",
    "        'p_value': [p_value]\n",
    "    })\n",
    "    \n",
    "    # Add coefficients for each independent variable\n",
    "    for i, var in enumerate(independent):\n",
    "        temp_results[f'{var}_coeff'] = [model.params[var]]\n",
    "        temp_results[f'{var}_pvalue'] = [model.pvalues[var]]\n",
    "    \n",
    "    # Add intercept coefficient\n",
    "    # temp_results['intercept_coeff'] = [model.params['const']]\n",
    "    # temp_results['intercept_pvalue'] = [model.pvalues['const']]\n",
    "    \n",
    "    if results.empty:\n",
    "        results = temp_results\n",
    "    else:\n",
    "        results = pd.concat([results, temp_results], ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = final_df[final_df.Coin.isin(['ORCAUSDT','BTCUSDT'])][['Coin','dateTime','close_pct']].pivot_table(index = 'dateTime',columns='Coin',values='close_pct')\n",
    "comparison = comparison[comparison.index>min_date]\n",
    "print(comparison.corr())\n",
    "comparison.plot(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Get unique coins and create a color map\n",
    "unique_coins = selected_coins\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(unique_coins)))\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Scatter plot on the left\n",
    "for i, coin in enumerate(unique_coins):\n",
    "    coin_data = residuals_df[residuals_df['coin'] == coin]\n",
    "    ax1.scatter(coin_data['dateTime'], coin_data['residual'], \n",
    "               color=colors[i], label=coin, alpha=0.7, s=20)\n",
    "\n",
    "ax1.set_xlabel('Date Time')\n",
    "ax1.set_ylabel('Residuals')\n",
    "ax1.set_title('Residuals by Coin Over Time')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Histogram on the right\n",
    "for i, coin in enumerate(unique_coins):\n",
    "    coin_data = residuals_df[residuals_df['coin'] == coin]\n",
    "    ax2.hist(coin_data['residual'], bins=30, alpha=0.5, \n",
    "            color=colors[i], label=coin, density=True)\n",
    "\n",
    "ax2.set_xlabel('Residual Value')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Distribution of Residuals by Coin')\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (\n",
    "    residuals_df\n",
    "    .groupby('coin')\n",
    "    .agg({\n",
    "        'residual':['mean', 'std'],\n",
    "        'dateTime':'nunique'\n",
    "    })\n",
    ")\n",
    "result.columns = ['_'.join(col).strip() for col in result.columns.values]\n",
    "# Calculate p-values for each coin using one-sample t-test\n",
    "from scipy.stats import ttest_1samp\n",
    "import scipy.stats as scipy_stats\n",
    "import numpy as np\n",
    "\n",
    "result_with_pvalues = result.copy()\n",
    "p_values = []\n",
    "\n",
    "for coin in result.index:\n",
    "    coin_residuals = residuals_df[residuals_df['coin'] == coin]['residual']\n",
    "    \n",
    "    # Use the method from file_context_0\n",
    "    mean = coin_residuals.mean()\n",
    "    threshold = 0\n",
    "    n = len(coin_residuals)\n",
    "    sd = coin_residuals.std()\n",
    "    \n",
    "    # Calculate t-statistic\n",
    "    t_stat = (mean - threshold) / (sd / np.sqrt(n))\n",
    "    \n",
    "    # Calculate p-value based on test type\n",
    "    # One-sided test: H0: mean <= threshold, H1: mean > threshold\n",
    "    p_value = 1 - scipy_stats.t.cdf(t_stat, df=n-1)\n",
    "    \n",
    "    p_values.append(p_value)\n",
    "\n",
    "result_with_pvalues['p_value'] = p_values\n",
    "result_with_pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = model.summary()\n",
    "print(summary)\n",
    "# Get key statistics from the summary\n",
    "print(\"R-squared:\", model.rsquared)\n",
    "print(\"Adjusted R-squared:\", model.rsquared_adj)\n",
    "print(\"F-statistic:\", model.fvalue)\n",
    "print(\"F-statistic p-value:\", model.f_pvalue)\n",
    "print(\"AIC:\", model.aic)\n",
    "print(\"BIC:\", model.bic)\n",
    "\n",
    "# Get coefficient information\n",
    "print(\"\\nCoefficients:\")\n",
    "print(model.params)\n",
    "print(\"\\nP-values:\")\n",
    "print(model.pvalues)\n",
    "print(\"\\nStandard errors:\")\n",
    "print(model.bse)\n",
    "print(\"\\nConfidence intervals:\")\n",
    "print(model.conf_int())\n",
    "\n",
    "# Get residual statistics\n",
    "print(\"\\nResidual statistics:\")\n",
    "print(\"Mean of residuals:\", model.resid.mean())\n",
    "print(\"Standard deviation of residuals:\", model.resid.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTC LAGGED ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_vars.Coin.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction_vars = final_df[final_df.Coin.isin(independent)][['dateTime', 'Coin', 'close_pct']]\n",
    "\n",
    "def make_lagged_columns(df, lag_from = 1, lag_to = 5):\n",
    "    temp = pd.DataFrame()\n",
    "    temp['dateTime'] = df.dateTime.unique()\n",
    "    for coin in df.Coin.unique():\n",
    "        coin_df = df[df.Coin == coin][['dateTime','close_pct']]\n",
    "        for i in range(lag_from, lag_to+1):\n",
    "            temp[f'lag_{coin}_{i}'] = coin_df['close_pct'].shift(i).values\n",
    "            \n",
    "    return temp\n",
    "lagged_features = make_lagged_columns(prediction_vars, 1, 100)\n",
    "lagged_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joined_df = final_df[['Coin','dateTime','close_pct']].merge(lagged_features, on = 'dateTime', how = 'right').fillna(0)\n",
    "joined_df\n",
    "coin = 'PENDLEUSDT'\n",
    "corr = 'BTCUSDT'\n",
    "correlations = pd.DataFrame()\n",
    "for coin in tqdm(joined_df.Coin.unique()):\n",
    "    for corr in prediction_vars.Coin.unique():\n",
    "        result = joined_df[joined_df.Coin == coin]\n",
    "        result = result[['close_pct']+[col for col in result.columns if corr in col]].corr()['close_pct'].reset_index()\n",
    "        result = result[result.close_pct < 1]\n",
    "        result['index'] = result['index'].apply(lambda x: x.split('_')[2])\n",
    "        result['Coin'] = coin\n",
    "        result['Corr'] = corr\n",
    "        correlations = pd.concat([correlations, result])\n",
    "\n",
    "correlations\n",
    "# for coin in joined_df.Coin.unique()[0:2]:\n",
    "#     coin_df = joined_df[joined_df.Coin == coin]\n",
    "#     print(coin_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with 4 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "fig.suptitle('Time Series Correlation Analysis', fontsize=16)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot for each reference coin\n",
    "for idx, corr in enumerate(['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'XRPUSDT']):\n",
    "    # Get data for this reference coin\n",
    "    corr_data = correlations[correlations.Corr == corr]    \n",
    "    # Plot each coin's correlation as a line\n",
    "    for coin in corr_data.Coin.unique():\n",
    "        coin_data = corr_data[corr_data.Coin == coin]\n",
    "        if abs(coin_data.close_pct.mean()) > 0.01:\n",
    "            axes[idx].scatter(coin_data['index'], coin_data['close_pct'], \n",
    "                      label=coin, alpha=0.1)\n",
    "    \n",
    "    axes[idx].set_title(f'Correlation with {corr}')\n",
    "    axes[idx].set_xlabel('Lag')\n",
    "    axes[idx].set_ylabel('Correlation')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    # axes[idx].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Set x-ticks to show every 5th tick\n",
    "    x_ticks = axes[idx].get_xticks()\n",
    "    axes[idx].set_xticks(x_ticks[::5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin = 'PENDLEUSDT'\n",
    "coin_df = final_df[final_df.Coin == coin][['dateTime','close','close_pct']]\n",
    "feature_df = coin_df.merge(lagged_features, on = 'dateTime', how = 'right').fillna(0)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import mlflow.lightgbm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Start MLflow experiment\n",
    "mlflow.set_experiment(f\"Crypto_Price_Prediction_{coin}\")\n",
    "\n",
    "# Define models and their parameter grids - simplified to avoid multiprocessing issues\n",
    "models_and_params = {\n",
    "    'OLS': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {}\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'model': DecisionTreeRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'max_depth': [5, 10, 15],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestRegressor(random_state=42, n_jobs=1),  # Single thread\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [5, 10],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': xgb.XGBRegressor(random_state=42, verbosity=0, n_jobs=1),  # Single thread\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 6],\n",
    "            'learning_rate': [0.01, 0.1]\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': lgb.LGBMRegressor(random_state=42, verbosity=-1, n_jobs=1),  # Single thread\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 6],\n",
    "            'learning_rate': [0.01, 0.1]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "y = feature_df['close_pct']\n",
    "X = feature_df[[col for col in feature_df.columns if 'lag' in col]]\n",
    "\n",
    "# Remove rows with NaN values\n",
    "mask = ~(y.isna() | X.isna().any(axis=1))\n",
    "y_clean = y[mask]\n",
    "X_clean = X[mask]\n",
    "\n",
    "# Scale features for models that need it\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "\n",
    "# Train-test split (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Grid search for each model\n",
    "for model_name, model_config in models_and_params.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{coin}_{model_name}\"):\n",
    "        # Log basic info\n",
    "        mlflow.log_param(\"coin\", coin)\n",
    "        mlflow.log_param(\"model_type\", model_name)\n",
    "        mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "        mlflow.log_param(\"train_samples\", len(X_train))\n",
    "        mlflow.log_param(\"test_samples\", len(X_test))\n",
    "        \n",
    "        # Perform grid search with cross-validation - single threaded to avoid multiprocessing issues\n",
    "        if model_config['params']:  # If there are parameters to tune\n",
    "            grid_search = GridSearchCV(\n",
    "                model_config['model'], \n",
    "                model_config['params'], \n",
    "                cv=3,  # Reduced CV folds\n",
    "                scoring='neg_mean_squared_error',\n",
    "                n_jobs=1,  # Single thread\n",
    "                verbose=0\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            \n",
    "            # Log best parameters\n",
    "            for param, value in grid_search.best_params_.items():\n",
    "                mlflow.log_param(f\"best_{param}\", value)\n",
    "            \n",
    "            # Log CV score\n",
    "            mlflow.log_metric(\"cv_score\", -grid_search.best_score_)\n",
    "            \n",
    "        else:  # For OLS (no parameters to tune)\n",
    "            best_model = model_config['model']\n",
    "            best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_train = best_model.predict(X_train)\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_r2 = r2_score(y_train, y_pred_train)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "        test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"train_r2\", train_r2)\n",
    "        mlflow.log_metric(\"test_r2\", test_r2)\n",
    "        mlflow.log_metric(\"train_mse\", train_mse)\n",
    "        mlflow.log_metric(\"test_mse\", test_mse)\n",
    "        mlflow.log_metric(\"train_mae\", train_mae)\n",
    "        mlflow.log_metric(\"test_mae\", test_mae)\n",
    "        mlflow.log_metric(\"train_rmse\", np.sqrt(train_mse))\n",
    "        mlflow.log_metric(\"test_rmse\", np.sqrt(test_mse))\n",
    "        \n",
    "        # Log model with input example to avoid warnings\n",
    "        input_example = X_train.iloc[:5] if hasattr(X_train, 'iloc') else X_train[:5]\n",
    "        \n",
    "        if model_name == 'XGBoost':\n",
    "            mlflow.xgboost.log_model(best_model, \"model\", input_example=input_example)\n",
    "        elif model_name == 'LightGBM':\n",
    "            mlflow.lightgbm.log_model(best_model, \"model\", input_example=input_example)\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(best_model, \"model\", input_example=input_example)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'train_mse': train_mse,\n",
    "            'test_mse': test_mse,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'best_model': best_model\n",
    "        })\n",
    "        \n",
    "        print(f\"{model_name} - Test R²: {test_r2:.4f}, Test MSE: {test_mse:.4f}, Test MAE: {test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resid\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.kdeplot(model.resid, )\n",
    "plt.title('Kernel Density Plot of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
